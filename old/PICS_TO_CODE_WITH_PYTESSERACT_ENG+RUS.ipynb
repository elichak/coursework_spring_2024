{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2cd35020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import dask.bag as db\n",
    "import re\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a70724ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_files = os.listdir('roles_okved_model')\n",
    "name_files = list(map(lambda x: 'roles_okved_model/' + x, name_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "47d0957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_pic_to_text(pic):\n",
    "    return pytesseract.image_to_string(pic, lang='eng+rus') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9623e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pics = list(map(cv2.imread, name_files))\n",
    "text = list(map(func_pic_to_text, pics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2e440793",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = ''.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d701196",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e01a6c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "№! /usr/bin/env python\n",
      "# coding: utf-8\n",
      "\n",
      "# In[1]:\n",
      "\n",
      "get_ipython().run_line_magic(*load_ext', 'autoreload'\n",
      "get_ipython().run_line_magic('autoreload', '2°\n",
      "\n",
      "# In[2]:\n",
      "\n",
      "import. json\n",
      "from pathlib import Path\n",
      "import pickle\n",
      "\n",
      "from os.path import isfile\n",
      "\n",
      "from gensim.models import Word2Vec\n",
      "from gensim.utils import simple_preprocess\n",
      "from 1xm1 import etree\n",
      "\n",
      "import numpy as пр\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "from tgdm.notebook import tqdm\n",
      "\n",
      "import yam]\n",
      "\n",
      "import zipfile\n",
      "\n",
      "# In[3]:\n",
      "\n",
      "def generate_financial_features(fin_features: pd.DateFrame,\n",
      "financial_cols: list,\n",
      "q_threshold: float = .5) -> pd.DataFrame\n",
      "“'Ocrasnaer от финансовых показателей fin_features данные для каждой компании за последний год\n",
      "и удаляет столбцы, в которых доля нулей больше g_threshold\n",
      "\n",
      "Args:\n",
      "fin_features (pd.DataFrame): Таблица с финансовыми показателями компаний за несколько лет\n",
      "financial_cols (list): Список идентификаторов рассматриваемых финансовых показателей без суффиксов 3 и 4\n",
      "q_threshold (float, optional): Порог отсечения, который используется для удаления\n",
      "колонок, содержащих большое кол-во нулей\n",
      "\n",
      "Returns:\n",
      "pd.DataFreme: Таблица < финансовыми показателями компаний за\n",
      "последний год и без столбцов, содержащих большое кол-во\n",
      "пропусков\n",
      "\n",
      " \n",
      "\n",
      "possible_fin_statement_cols\n",
      "for col in financial_cols:\n",
      "possible_fin_statement_cols.extend([f'p{col}3', f*p{col}4']\n",
      "\n",
      "ia]\n",
      "\n",
      "#in_features.sort_values([*arango_id', 'balance_year'], ascending-False, inplace=True)\n",
      "\f",
      "fin_features.drop_duplicates(['*arango_id'], inplace=True)\n",
      "fin_features.set_index('arango_id', inplace=True)\n",
      "\n",
      "zeros_by_col = (fin_features[possible_fin_statement_cols] == @).sum(axis=0)\n",
      "fin_statement_cols = zeros_by col[zeros_by_col <= zeros_by_col.quantile(q threshold) ].index\n",
      "fin_statements_df = fin_features[fin_statement_cols]\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "return fin_statements_df\n",
      "\n",
      "# In[4]:\n",
      "\n",
      "def generate_bankrupt_features(bankrupt_features: pd.DataFrame) -> pd.DataFrame:\n",
      "*''Фильтрует строки bankrupt_features, оставляя данные по банкротствам для\n",
      "каждой компании за последний год\n",
      "\n",
      "Arg\n",
      "\n",
      " \n",
      "\n",
      "bankrupt_features (pd.DataFrame): Таблица с данными по банкротствам компаний за несколько лет\n",
      "\n",
      " \n",
      "\n",
      "Return:\n",
      "pd.DataFrame: Таблица < данными по банкротствам компаний за последний год\n",
      "\n",
      "bankrupt_features| 'bankrupt_status_date'] = bankrupt_features[ 'bankrupt_status_date'].astype(str).str.slice(®, -6)\n",
      "bankrupt_features| 'bankrupt_status_date'] = pd.to_datetime(bankrupt_features| 'bankrupt_status_date'])\n",
      "bankrupt_features.sort_values(['arango_id', 'bankrupt_status_date'], ascending-False, inplace=True)\n",
      "bankrupt_features.drop_duplicates(['arango_id'], inplace-True)\n",
      "\n",
      "bankrupt_features.set_index('arango_id', inplace=True)\n",
      "\n",
      "bankrupt_features = bankrupt_features[[“bankrupt_status' ]]\n",
      "\n",
      "return bankrupt_features\n",
      "\n",
      "# In[5]:\n",
      "\n",
      "def get_links_with_fixed_directions(raw_links: list, directions: dict) -> list:\n",
      "Добавляет обратные связи для типов отношений, где это имеет смысл. Убирает суффикс _сору из названий типов.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Arg:\n",
      "raw_links (list[dict]): список связей между узлами\n",
      "directions (dict[dict]): словарь, содержащий информацию об ориентированности связей\n",
      "\n",
      " \n",
      "\n",
      "Return:\n",
      "list[dict]:\n",
      "\n",
      " \n",
      "\n",
      "список связей между узлами, расширенный за счет добавления обратных связей\n",
      "Links = []\n",
      "for link in raw_links\n",
      "key = link[*key'].rstrip('_copy')\n",
      "assert key in directions['directed'] or key in directions['undirected'], f'missing {key}'\n",
      "if key in directions[ 'undirected' ]:\n",
      "reversed _link = {'source': link['target'], 'target'\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "     \n",
      "\n",
      "link[source'], 'key': key}\n",
      "\f",
      "   \n",
      "\n",
      "Links. append({'source': link['source'], 'target': link['target'], 'key': key})\n",
      "\n",
      "return links\n",
      "\n",
      "# In[6]:\n",
      "\n",
      " \n",
      "\n",
      "def merge_and_fill(nodes_date: dict,\n",
      "fin_features: pd.DataFrame,\n",
      "bankrupt_features: pd.DateFrame,\n",
      "fill_values: dict) -> dict:\n",
      "** 'Объединяет 3 источника данных: информация об узлах ГСК, финансовые показатели компаний и\n",
      "данные о банкротствах компаний. Заполняет пропуски в данных.\n",
      "\n",
      " \n",
      "\n",
      "Arg:\n",
      "\n",
      "nodes_data (dict): информация об узлах из файла < ГСК\n",
      "\n",
      "fin_features (pd.DataFrame): таблица с финансовыми показателями компаний\n",
      "\n",
      "bankrupt_features (pd.DateFrame): таблица с информацией о банкротствах компаний\n",
      "\n",
      "fill_values (dict): словарь со значениями для заполнения пропусков в столбцах определенных типов\n",
      "Returns:\n",
      "list[dict\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "обогащенная за счет всех имеющихся источников информация об узлах из файла с ГСК\n",
      "\n",
      " \n",
      "\n",
      "nodes_data = (pd.DataFrame(nodes_data)\n",
      "-merge(fin_features, how='left', left_on:\n",
      "-merge(bankrupt_features, how='left', left_o\n",
      "for type_ in [object, int, float]:\n",
      "cols = nodes_data.select_dtypes(type_).columns\n",
      "nodes_data[cols] = nodes_data[cols].fillna(fill_values[type\n",
      "return nodes_data.to_dict(orient='records')\n",
      "\n",
      "     \n",
      "\n",
      "id', right_index-True)\n",
      "'id', right_index-True))\n",
      "\n",
      " \n",
      "\n",
      "._ папе__])\n",
      "\n",
      " \n",
      "\n",
      "# тп [7]:\n",
      "\n",
      " \n",
      "\n",
      "def augment_dataset(dir_load: str,\n",
      "dir_save: str,\n",
      "fin_features: pd.DataFrame,\n",
      "bankrupt_features: pd.DataFreme,\n",
      "directions: dict,\n",
      "fill_values: dict) -> None:\n",
      "“''3arpyxaet и предобрабатывает данные о ГСК: добавляет узлам новые атрибуты из других источников данных,\n",
      "заполняет пропуски, добавляет обратные связи (где это корректно). Сохраняет результат в pickle.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Arg\n",
      "\n",
      "  \n",
      "\n",
      "dir_load (str): путь к каталогу ¢ JSON файлами, содержащими информацию о ГСК\n",
      "dir_save (str): путь к каталогу для сохранения преобработанных файлов\n",
      "\n",
      "fin_features (pd.DataFrame): таблица с финансовыми показателями компаний\n",
      "\n",
      "bankrupt_features (pd.DataFrame): таблица с информацией о банкротствах компаний\n",
      "\n",
      "directions (dict[dict]): словарь, содержащий информацию об ориентированности связей\n",
      "\n",
      "fill_values (dict): словарь со значениями для заполнения пропусков в столбцах определенных типов\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "for fname in tqdm(1ist(Path(dir_load).iterdir())\n",
      "with open(fname, 'г', encoding='utf8') as fp\n",
      "\n",
      "   \n",
      "\f",
      "data = json.load(fp)\n",
      "\n",
      "if 'graph' not in data and 'ГСК гост” in data:\n",
      "data['graph'] = {*root': data.pop('ICK_root')}\n",
      "\n",
      " \n",
      "\n",
      "data[ 'links' ]\n",
      "datal nodes' ]\n",
      "\n",
      "get_links_with_fixed_directions(data['links*], directions)\n",
      "merge_and_fill(data['nodes'], fin_features, bankrupt_features, fill_values)\n",
      "\n",
      "if not data['nodes*] or not data[ 'links' ]:\n",
      "continue\n",
      "\n",
      "with open(Path(dir_save) / fname.with_suffix('.pickle').name, 'wb') as fp:\n",
      "pickle.dump(date, fp)\n",
      "\n",
      "# In[8]:\n",
      "\n",
      "def get_name_description(text: str) -> tuple:\n",
      "“''Tlenut длинную строку с названием кода и описанием на 2 части\n",
      "\n",
      " \n",
      "\n",
      "Args:\n",
      "text (str): строка с названием и описаниям вроде 'Выращивание специй [...] Эта группировка включает [..\n",
      "\n",
      " \n",
      "\n",
      "Returns:\n",
      "tuple: Koptex с названием кода и его подробным описанием (если оно имеется)\n",
      "\n",
      "# описание начинается со слова 'эта\n",
      "idx = text.lower().find('sTa')\n",
      "if idx > -1\n",
      "\n",
      "return (text[:idx], text[idx:])\n",
      "return (text, None)\n",
      "\n",
      "def split_okved(s: str) -> tuple:\n",
      "“'*Pas6usaer код ОКВЭД на составляющие: класс, подкласс, группу, подгруппу и тип\n",
      "\n",
      "Args:\n",
      "$ (str): код ОКВЭД\n",
      "\n",
      "Return:\n",
      "tuple: кортеж с классом, подклассом, группой, подгруппой и типом кода\n",
      "\n",
      " \n",
      "\n",
      "class_, subclass, group, subgroup, type_ = [None] * 5\n",
      "assert len(s) in {2, 4, 5, 7, 8}\n",
      "\n",
      "class_ = s[:2]\n",
      "\n",
      "if len(s) >= 4:\n",
      "subclass = s[:4]\n",
      "\n",
      "if len(s) >= 5:\n",
      "group = s[:5]\n",
      "\n",
      "if len(s) >= 7:\n",
      "\f",
      " \n",
      "\n",
      "if len(s) >= 7:\n",
      "subgroup\n",
      "if len(s)\n",
      "type_=s\n",
      "\n",
      "$[:7]\n",
      "\n",
      " \n",
      "\n",
      "8\n",
      "\n",
      " \n",
      "\n",
      "return class_, subclass, group, subgroup, type_\n",
      "\n",
      "def get_word2vec_embeddings (descriptions: pd.Series, vector_size: int) -> np.array:\n",
      "“''Crpowr эмбеддинги для текстового описания кодов ОКВЭД\n",
      "\n",
      "'Args:\n",
      "descriptions (pd.Series): серия с описаниями всех кодов\n",
      "vector_size (int): размерность эмбеддингов описаний ОКВЭД\n",
      "\n",
      "Returns:\n",
      "пр-аггау: массив эмбеддингов описания каждого из кодов\n",
      "\n",
      "sents = (descriptions.str.replace(r*(\\w)([A-A])', '\\\\1 \\\\2', regex=True)\n",
      "-str.replace(r'[*A-fla-a\\d\\s]', '', regex=True)\n",
      "-map(simple_preprocess)).tolist()\n",
      "\n",
      "model = Word2Vec(sents, min_count=1, vector_size=vector_size)\n",
      "\n",
      "embeddings = np.array([model.wv[sent].mean(axis=@) for sent in sents])\n",
      "\n",
      "return embeddings\n",
      "\n",
      "def read_docx(docx_file: str, **kwargs) -> list:\n",
      "'Читает файл формата docx и вытаскивает оттуда таблицы\n",
      "\n",
      "'Args:\n",
      "docx_file (str): путь к docx файлу\n",
      "\n",
      "Returns:\n",
      "list[pd.DataFrame]: список таблиц, находящихся в файле\n",
      "ns = {'ы': 'http: //schemas. openxml formats .org/wordprocessingml/2006/main' }\n",
      "with zipfile.ZipFile(docx_file).open('word/document.xml') аз +:\n",
      "root = etree.parse(f)\n",
      "for el in root.xpath('//w:tbl', namespaces=ns):\n",
      "el.tag = 'table'\n",
      "for el in root.xpath('//w:tr', namespaces=ns):\n",
      "\n",
      "el.tag = 'tr'\n",
      "for el in root.xpath('//w:tc', namespaces=ns):\n",
      "el.tag = 'td'\n",
      "\n",
      "return pd.read_html(etree.tostring(root), **kwargs)\n",
      "\n",
      "def create_okved_df(doc_path: str, vector_size: int = 64) -> pd.DataFrame:\n",
      "'Создает DataFrame с информацией о кодах ОКВЭД\n",
      "\n",
      "Args:\n",
      "docx_file (str): путь к docx файлу (закону об ОКВЭД)\n",
      "\n",
      "vector_size (int, optional): размерность эмбеддингов описаний ОКВЭД\n",
      "\n",
      "Returns:\n",
      "\f",
      "Returns:\n",
      "pd.DataFrame: таблица < информацией об ОКВЭД\n",
      "\n",
      " \n",
      "\n",
      "okved_doc_dfs = read_docx(doc_path)\n",
      "\n",
      "# таблица с ОКВЭД лежит под индексом 8\n",
      "\n",
      "okved_data = okved_doc_d¥s[8].rename(columns={@: 'okved' ,\n",
      "\n",
      "# заполняем столбец с разделом\n",
      "\n",
      "curr = None\n",
      "\n",
      "for idx, row in okved_data.iterrows():\n",
      "if 'pasgen' in rou[ 'okved'].lower():\n",
      "\n",
      "curr = row['okved'].title()\n",
      "\n",
      "okved_data.at[idx, 'раздел”] = curr\n",
      "\n",
      " \n",
      "\n",
      "*description_full'}) .dropna(subset=[ 'okved'])\n",
      "\n",
      " \n",
      "\n",
      "# удаляем строки без кодов ОКВЭД\n",
      "okved_data = okved_data[okved_datal “окуеа' ].5'.патев(г'\\4{2}\\.?\\9{0,2}\\.?\\{0,2}')]\n",
      "okved_data[['name', 'description']] = okved_datal 'description_full' ].map(get_name_description).tolist()\n",
      "\n",
      " \n",
      "\n",
      "okved_parts = ['okved_class_', 'okved_subclass', 'okved_group', 'okved_subgroup', 'okved_type\n",
      "okved_data[okved_parts] = okved_datal 'okved'].map(split_okved).tolist()\n",
      "\n",
      " \n",
      "\n",
      "embeddings = get_word2vec_embeddings(okved_data[ 'description_full'], vector_size)\n",
      "okved_data.loc[:, [f*x_{i}' for i in range(vector_size)]] = embeddings.tolist()\n",
      "okved_data = okved_data.append([{'okved': 'root', 'name': 'Корень классификатора'} ])\n",
      "okved_data.reset_index(drop=True, inplace=True)\n",
      "\n",
      "okved_data.index.name = 'okved_id*\n",
      "\n",
      "return okved_data\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "# In[{11):\n",
      "\n",
      " \n",
      "\n",
      "# загружаем конфигурационный файл\n",
      "CONFIG = yaml.safe_load(open(*CONFIG.yaml', encoding='utf8'))\n",
      "fin_data_path = Path(CONFIG[ 'paths']['fin_data'])\n",
      "bankrupt_date_path = Path(CONFIG[ 'paths' ][ “bankrupt_data'])\n",
      "\n",
      "print('Start...')\n",
      "\n",
      "# создаем таблицу с финансовыми показателями\n",
      "\n",
      "if isfile(fin_data_path.with_suffix(' .ргергосеззед.с5м')):\n",
      "fin_features = pd.read_csv(fin_data_path.with_suffix(\n",
      "\n",
      "else\n",
      "fin_features = pd.read_csv(fin_data_path)\n",
      "fin_features = generate_financial_features(fin_features, financial_cols-CONFIG| ' financial' ])\n",
      "fin_features.to_csv(fin_data_path.with_suffix('.preprocessed.csv'))\n",
      "\n",
      "print('fin_features created...')\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "-preprocessed.csv'), index_col=0)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "# создаем таблицу с информацией о банкротстве\n",
      "if isfile(bankrupt_data_path.with_suffix(' .ргергосеззей.сзу')\n",
      "\n",
      "bankrupt_features = pd.read_csv(bankrupt_date_path.with_suffix('.preprocessed.csv'), index_col=0)\n",
      "else:\n",
      "\n",
      "bankrupt_features = pd.read_csv(bankrupt_date_path)\n",
      "\n",
      "bankrupt features = generate bankrupt features(bankrupt features)\n",
      "\n",
      " \n",
      "\f",
      "bankrupt_features.to_csv(bankrupt_data_path.with_suffix('.preprocessed.csv'))\n",
      "print('bankrupt_features created...')\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "# создаем расширенный датасет на основе всех источников данных\n",
      "augment_dataset (CONFIG[ 'paths* ] ['вс_дата'],\n",
      "CONFIG[ 'paths' ][“gc_augmented_save'],\n",
      "fin_features,\n",
      "bankrupt_features,\n",
      "directions-CONFIG[ 'directions' ],\n",
      "fi11_values=CONFIG[ 'constants' ]['fill_value'])\n",
      "\n",
      "  \n",
      "\n",
      "# создаем таблицу с информацией об ОКВЭД Ha основе закона\n",
      "print('augmented dataset created...')\n",
      "\n",
      "okved_df = create_okved_df(CONFIG[ 'paths*][ 'okved_doc'])\n",
      "okved_df.to_csv(CONFIG[ 'paths' ][ 'okved_data_save'])\n",
      "print(“okved data created...')\n",
      "\n",
      "print('Done”)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "}H!/usr/bin/env python\n",
      "# coding: utf-8\n",
      "\n",
      " \n",
      "\n",
      "# In[1]:\n",
      "\n",
      "get_ipython().run_line_magic(*load_ext', 'autoreload')\n",
      "get_ipython().run_line_magic('autoreload', *2°)\n",
      "from imports import *\n",
      "\n",
      "# In[2]:\n",
      "\n",
      "def binarize_int(idx: int, n_bits: int) -> lis\n",
      "“''TlepesoauT целое число в двоичную систему < заданным кол-вом бит и представляет\n",
      "результат в виде списка\n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "Arg\n",
      "\n",
      " \n",
      "\n",
      "idx (int\n",
      "n_bits (int\n",
      "\n",
      "целое число для преобразования\n",
      "кол-во бит для хранения двоичного числа\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Return:\n",
      "list[int]: представление числа в виде списка из n_bits нулей и единиц\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "return [int(c) for ¢ in format(idx, f'#@{n_bits+2}b')[\n",
      "\n",
      " \n",
      "\n",
      "1\n",
      "\n",
      "# In[3]:\n",
      "\n",
      "def read_node_features_with_gc(dir_: str, cols: list) -> pd.DataFrame:\n",
      "*''Получает таблицу со всеми значениями атрибутов всех узлов\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Arg:\n",
      "dir_ (str): путь к каталогу с pickle файлами с информацией о ГСК\n",
      "cols (list[str]): список атрибутов для выбора\n",
      "\n",
      " \n",
      "\n",
      "Return:\n",
      "pd.DataFrame: таблица < информацией обо всех компаний в датасете\n",
      "features = []\n",
      "for fname in list(Path(dir_).iterdir()):\n",
      "with open(fname, 'rb') as fp:\n",
      "data = pickle. load(fp)\n",
      "for item in data[ 'nodes' ]\n",
      "item = {k: v for К, v in item.items() if К in cols}\n",
      "item['TCK'] = fname.stem\n",
      "features. append(item)\n",
      "df = pd.DataFrame(features)\n",
      "d¥.drop_duplicates(*id', inplace-True)\n",
      "d¥.set_index(*id', inplace-True)\n",
      "return df\n",
      "\n",
      " \n",
      "\f",
      "def read_edge_features(dir_: str) -> pd.DataFrame:\n",
      "“''TlonysaeT фрейм co всеми значениями атрибутов связей (для дальнейшего кодирования)\n",
      "\n",
      "Args:\n",
      "dir_ (str): путь к каталогу с pickle файлами с информацией о ГСК\n",
      "\n",
      "Returns:\n",
      "pd.DataFrame: таблица < информацией обо всех связях в датасете\n",
      "features = []\n",
      "for fname in list(Path(dir_).iterdir()):\n",
      "with open(fname, 'rb') as fp:\n",
      "data = pickle. load(fp)\n",
      "features .extend(data[ 'links*])\n",
      "df = pd.DataFrame(features)\n",
      "return df\n",
      "\n",
      " \n",
      "\n",
      "# In[4]:\n",
      "\n",
      "def push_reverse_eid_to_end(edge_df: pd.DataFrame) -> pd.DataFrame:\n",
      "*''Сортирует ребра Tak, чтобы взаимообратные ребра находились на расстоянии len(edge df) // 2\n",
      "\n",
      "Args:\n",
      "edge_df (pd.DateFrame): таблица с ребрами\n",
      "\n",
      "Returns:\n",
      "pd.DataFrame: таблица < ребрами отсортированными так, чтобы взаимообратные ребра\n",
      "находились на расстоянии len(edge df) // 2\n",
      "edge_df = edge_df.copy()\n",
      "edges_sort = {}\n",
      "curr_idx = 0\n",
      "edge_to_eid = {(u, у): eid for eid, (и, м) in enumerate(edge_df[['u', 'v']]-values)}\n",
      "for (и, м) in edge_df[['u', 'v']].values:\n",
      "if edge_to_eid[(u, v)] not in edges_sort:\n",
      "edges_sort[edge_to_eid[(u, v)]] = curr_idx\n",
      "edges_sort[edge_to_eid[(v, u)]] = curr_idx + len(edge_df) // 2\n",
      "curr_idx += 1\n",
      "edge_df['order'] = edge_df.index.map(edges_sort)\n",
      "edge_df = edge_df.sort_values('order').reset_index(drop=True) .drop(columns=' order”)\n",
      "return edge df\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "def create_ge_edges(company_df: pd.DataFrame,\n",
      "company _edges_df: pd.DataFrame,\n",
      "okved_to_idx: dict) -> pd.DataFrame:\n",
      "'Создает таблицу с ребрами на основе данных из ГСК\n",
      "\n",
      "Args:\n",
      "company _df (pd.DataFrame): таблица < данными о компаниях\n",
      "company _edges_df (pd.DataFrame): таблица с данными о связях между компаниями\n",
      "okved_to_idx (dict): маппинг код ответ - номер кода\n",
      "Returns:\n",
      "pd.DataFrame: таблица < ребрами между кодами ОКВЭД на основе данных о ГСК\n",
      "\n",
      "# ребра на основе ГСК\n",
      "# обратные связи тут не добавляем, они были добавлены на этапе препроцессинга\n",
      "company _to_okved = сопрапу_9+['окуед_соде' ]\n",
      "\n",
      "# добавляем к company edges_d¥ информацию о ОКВЭД компаний, образующих ребро\n",
      "\n",
      "gc_edges_data = (сопрапу_едвез_а+\n",
      "-merge(company_to_okved, left_on='source', right_index-True)\n",
      "-merge(company_to_okved, left_on='target', right_index-True)\n",
      "-loc[:, ['source', 'okved_code_x', 'target', 'okved_code_y', 'key']]\n",
      ")\n",
      "\n",
      "# пока считаем все типы одинаковыми и считаем кол-во ребер между парами ОКВЭДов\n",
      "\n",
      "gc_edges = gc_edges_data.groupby(['okved_code_x', 'okved_code_y']).size()\n",
      "\n",
      "# убираем петли\n",
      "\n",
      "вс едвез = gc_edges[gc_edges. index.get_level_values(1)\n",
      "\n",
      "gc_edges = gc_edges.to_frame(name='weight').reset_index()\n",
      "\n",
      "# характеристики для фильтрации ребер\n",
      "\n",
      "# 0.25 квантили весов ребер от обеих вершин на ребре\n",
      "\n",
      "вс едвез['а_м1'] = gc_edges.groupby('okved_code_x').transform(lambda x: x.quantile(@.25))[ 'weight' ]\n",
      "\n",
      "вс едвез['9_м]'] = gc_edges.groupby('okved_code_y').transform(lambda x: x.quantile(@.25))[ 'weight' ]\n",
      "\n",
      "# оставляем ребра с весами, большими 25% квантилей с обеих сторон\n",
      "\n",
      "# минимальное значение квантиля 1, такие тоже обрасываем (знак >)\n",
      "\n",
      "gc_edges = gc_edges.query('(weight > q_wi) & (weight > q_wj)').reset_index(drop=True)\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "gc_edges. index.get_level_values(@)]\n",
      "\n",
      " \n",
      "\n",
      "# маппим названия ОКВЭД на целые числа и получаем веса ребер\n",
      "\n",
      "edata_gc = (gc_edges[['okved_code_x', 'okved_code_y', 'weight']]\n",
      "~rename(columns={'okved_code_x': 'u_code', 'okved_code_y': 'м соде'}))\n",
      "\n",
      "edata_gc['u'] = edata_gc[ 'u_code'].map(okved_to_idx)\n",
      "\n",
      "edata_gc['v'] = edata_gc[ 'v_code'].map(okved_to_idx)\n",
      "\n",
      "# пересортируем ребра\n",
      "\n",
      "edata_gc = push_reverse_eid_to_end(edata_gc)\n",
      "\n",
      "half = len(edata_gc)//2\n",
      "\n",
      "assert np.all(edata_gc[:half][['u', 'v']].values\n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "edata_gc[half: ][['v', 'u']]-values)\n",
      "\n",
      "return edata_gc\n",
      "\f",
      "def create_classifier_edges(okved_data: pd.DataFrame,\n",
      "okved_parts: list,\n",
      "gc_edges: pd.DataFrame,\n",
      "okved_to_idx: dict) -> pd.DataFrame:\n",
      "“'*Cosgaer таблицу с ребрами Ha основе данных из Классификатора\n",
      "\n",
      " \n",
      "\n",
      "Args:\n",
      "okved_data (pd.DataFrame): таблица с данными о кодах ОКВЭД\n",
      "okved_parts (list): список столбцов с 'частями' кодов\n",
      "gc_edges (pd.DataFrame): таблица с ребрами между кодами ОКВЭД на основе данных о ГСК\n",
      "okved_to_idx (dict): маппинг код ответ - номер кода\n",
      "\n",
      "Returns:\n",
      "pd.DataFrame: таблица < ребрами между кодами ОКВЭД на основе данных из Классификатора\n",
      "\n",
      "def get_classifier_weight(row, okved_tree):\n",
      "u, у = row['u_code'], гом[ 'v_code*]\n",
      "weight = okved_tree.edges[u, v]['weight']\n",
      "return weight\n",
      "\n",
      " \n",
      "\n",
      "# ребра на основе классификатора\n",
      "classifier_edges = set()\n",
      "# итерируемся по смежным частям ОКВЭДОВ и добавляем ребра\n",
      "for from_, to_ in zip(okved_parts, okved_parts[1:]):\n",
      "possible edges = okved_data[[from_, to_]].values\n",
      "checked_edges = possible_edges[(pd.notna(possible_edges[:, @])) &\n",
      "(pd.notna(possible_edges[:, 1]))]\n",
      "classifier_edges.update(tuple(t) for t in checked_edges.tolist())\n",
      "# добавляем обратные связи\n",
      "classifier_edges.update(tuple(t) for t in checked_edges[:,\n",
      "# ребра от и к корню\n",
      "classifier_edges.update(('root', у) for v in okved_datal 'okved_class_'].dropna().unique())\n",
      "classifier_edges.update((v, 'root') for м in okved_data| 'okved_class_'].dropna().unique())\n",
      "\n",
      " \n",
      "\n",
      "1-1]. tolist())\n",
      "\n",
      " \n",
      "\n",
      "# на основе ребер построим дерево классификатора\n",
      "okved_tree = nx.Graph()\n",
      "okved_tree.add_edges_from(classifier_edges)\n",
      "\n",
      "nx. set_edge_attributes(okved_tree, 1, 'weight')\n",
      "\n",
      " \n",
      "\n",
      "# на основе пар ОКВЭДОв из ГСК обновляем веса на ребрах классификатора\n",
      "# строим пути между парами и добавляем +1 на каждое ребро\n",
      "# тк работаем < деревом, кратчайший путь один\n",
      "for pair in gc_edges[['u_code', 'v_code']].values:\n",
      "\n",
      "path = nx.shortest_path(okved_tree, *pair)\n",
      "\n",
      "if len(path) > 2:\n",
      "\n",
      "for и, v in zip(path, path[1:]):\n",
      "okved_tree.edges[u, v]['weight'] += 1\n",
      "\n",
      "edata_classifier = pd.DataFrame(classifier_edges, columns=['u_code', 'v_code'])\n",
      "# маппим названия ОКВЭД на целые числа и получаем веса ребер из классификатора\n",
      "едата_с1азз1+1ег[ 'мезёве'] = edata_classifier.apply(get_classifier_weight, axis=\n",
      "edata_classifier['u'] = edata_classifier['u_code'].map(okved_to_idx)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "1, okved_tree-okved_tree)\n",
      "\n",
      " \n",
      "\f",
      "edata_classifier['v'] = edata_classifier['v_code*].map(okved_to_idx)\n",
      "# пересортируем ребра\n",
      "\n",
      "edata_classifier = push_reverse_eid_to_end(edata_clessifier)\n",
      "\n",
      "half = len(edata_classifier)//2\n",
      "\n",
      "assert np.all(edata_clessifier\n",
      "\n",
      " \n",
      "\n",
      "alf][['u', 'v']]-values\n",
      "\n",
      " \n",
      "\n",
      "edata_classifier[half:][['v', 'u']].values)\n",
      "\n",
      "return edata_clessifier\n",
      "\n",
      "# In[7]:\n",
      "\n",
      "def create_ndata(company_df: pd.DataFreme,\n",
      "okved_to_idx: dict,\n",
      "okved_data: pd.DataFrame,\n",
      "gc_edges: pd.DataFrame,\n",
      "okved_to_section: dict,\n",
      "bits_for_section: int = 5) -> pd.DataFrame:\n",
      "“'*CosaaeT таблицу с данными об узлах (кодах ОКВЭД)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Arg:\n",
      "company _df (pd.DataFrame): таблица с данными о компаниях\n",
      "okved_to_idx (dict): маппинг код ОКВЭД - номер кода\n",
      "okved_data (pd.DataFrame): таблица с данными о кодах ОКВЭД\n",
      "gc_edges (pd.DataFrame): таблица с ребрами между кодами ОКВЭД на основе данных о ГСК\n",
      "okved_to_section (dict): маппинг код раздела ОКВЭД - номер кода\n",
      "bits_for_section (int, optional): количество символов для кодирования секции\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Return:\n",
      "pd.DataFrame: таблица с данными об узлах\n",
      "\n",
      "def normalize(s\n",
      "return (s - s.mean()) / s.std()\n",
      "\n",
      " \n",
      "\n",
      "# фичи об ОКВЭДах: кол-во компаний + эмбеддинг описания\n",
      "ndata = pd.DataFrame({'okved_idx': okved_to_idx}) # уже выровнены по ОКВЭД\n",
      "# считаем количество компаний по ОКВЭДам\n",
      "пдата[ 'populerity'] = (сотрапу_4+.езет_1пдех()\n",
      "-groupby('okved_code')['id']\n",
      "-apply(len)\n",
      "-map(np. logip))\n",
      "embedding cols = [col for col in okved_data.columns if col.startswith('x_')]\n",
      "ndata = ndate.merge(okved_data[embedding cols], left_on='okved_idx', right_index-True, how='left').fillna(@.0)\n",
      "ndata[ 'out_degree'] = normalize(gc_edges.groupby(*u_code').size().rename('out_degree'))\n",
      "пдата['1п_девгее' ] = normalize(gc_edges.groupby('v_code').size().rename('in_degree'))\n",
      "ndata[ 'м4' ] = normalize(gc_edges.groupby('v_code*)[ 'weight' ].sum() .гепапе( 'м4 '))\n",
      "section_cols = [f'section_{i}' for i in range(bits_for_section)]\n",
      "ndata[section_cols] = ndata[ 'okved_idx'].map(okved_to_section).apply(binarize_int, n_bits-bits_for_section).tolist()\n",
      "ndata.fillna(@, inplace-True)\n",
      "return ndata\n",
      "\n",
      " \n",
      "\f",
      "def create_dgl_graph(edata_classifier: pd.DataFrame, edata_gc: pd.DataFrame, ndata: pd.DataFrame) -> dgl.DGLHeteroGraph:\n",
      "“'*CosgaeT граф на основе информации о ГСК и ОКВЭД\n",
      "\n",
      "Args:\n",
      "edata_classifier (pd.DataFrame): таблица с ребрами между кодами ОКВЭД на основе данных из Классификатора\n",
      "едата_вс (dict): таблица с ребрами между кодами ОКВЭД на основе данных о ГСК\n",
      "ndata (pd.DataFrame): таблица с данными об узлах\n",
      "\n",
      "Returns:\n",
      "\n",
      "dgl.DGLHeteroGraph: граф, в котором узлы - коды ОКВЭД,\n",
      "\n",
      "и имеются связи двух типов - на основе данных о ГСК и на основе Классификатора ОКВЭД\n",
      "\n",
      "# ребра от классификатора\n",
      "classifier_src = edata_clessifier['u'].values\n",
      "classifier_dst = edata_classifier['v'].values\n",
      "# ребра от ГСК\n",
      "gc_src = edata_ge['u'].values\n",
      "gc_dst = edata_ge['v'].values\n",
      "\n",
      " \n",
      "\n",
      "g_raw = dgl.heterograph({('okved', 'classifier', 'okved'): (classifier_src, classifier_dst),\n",
      "Cokved', 'вс', 'okved'): (gc_src, gc_dst)},\n",
      "num_nodes_dict={'okved': len(ndata)})\n",
      "\n",
      " \n",
      "  \n",
      "\n",
      "# добавляем фичи на ребра и узлы\n",
      "\n",
      "g_raw.ndata[ 'features' ] = torch.from_numpy(ndata.iloc[:, 1:].values)\n",
      "\n",
      "g_raw.edges[ 'classifier' ].data['weight'] = torch.FloatTensor(edata_classifier[ 'weight' ])\n",
      "g_raw.edges[ 'вс' ].data[ 'weight'] = torch.FloatTensor(edate_gc[ 'weight' ]. values)\n",
      "g_raw.ndatal 'okved_idx] = torch.from_numpy(ndata.iloc[:, @].values)\n",
      "\n",
      "в соппестед = dgl.node_subgraph(g_raw, (g_raw.in_degrees(etype='gc') > @ ).nonzero().flatten())\n",
      "\n",
      "half_n_edges = g_connected.num_edges('gc') // 2\n",
      "half_train_size = (half_n_edges) * 8 // 10\n",
      "\n",
      "half_perm = torch.randperm(half_n_edges)\n",
      "\n",
      "train_forward = half_perm[:half_train_size]\n",
      "train_reverse = train_forward + half_n_edges\n",
      "test_forward = half_perm[half_train_size:]\n",
      "test_reverse = test_forward + half_n_edges\n",
      "\n",
      "train_edges = torch.cat([train_forward, train_reverse])\n",
      "test_edges = torch.cat([test_forward, test_reverse])\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "g_connected.edges[ 'gc' ].data[ 'train_mask'] = torch.zeros(g_connected.num_edges('gc')).to(torch.bool)\n",
      "g_connected.edges[ 'gc' ].data[ 'train_mask' ][train_edges] = True\n",
      "\n",
      "g_connected.edges[ 'gc' ].data[ 'test_mask'] = torch.zeros(g_connected.num_edges(*gc')).to(torch.bool)\n",
      "g_connected.edges[ 'gc' ].data[ 'test_mask'][test_edges] = True\n",
      "\n",
      "return g_connected\n",
      "\f",
      "# загружаем конфигурационный файл\n",
      "\n",
      "CONFIG = yaml.safe_load(open( 'CONFIG. yaml', encoding='utf8'))\n",
      "node_features_path = CONFIG[ 'paths' ]['gc_augmented_node_features*]\n",
      "edge_features_path = CONFIG[ 'paths' ][“gc_augmented_edge_features']\n",
      "gc_augmented_path = CONFIG[ 'paths*]['gc_augmented_save']\n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "# создаем таблицу с информацией об атрибутах компаний\n",
      "if isfile(node_features_path):\n",
      "company_df = pd.read_csv(node_features_path, index_col=@)\n",
      "else:\n",
      "company_df\n",
      "\n",
      "read_node_features_with_gc(gc_augmented_path,\n",
      "\n",
      "cols-['id', “okved_code', *TCK*])\n",
      "company_df = company _df[company_df['okved_code*] != 'ипкпомп']\n",
      "company_df.to_csv(node_features_path)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "# создаем таблицу с информацией об атрибутах связей между компаниями\n",
      "if isfile(edge_features_path):\n",
      "company_edges_df = pd.read_csv(edge_features_path)\n",
      "else:\n",
      "company _edges_df = read_edge_features(gc_augmented_path)\n",
      "company _edges_d¥.to_csv(edge_features_path, index-False)\n",
      "\n",
      "# Оставляем только 5 типов отношений (самые частые, они еще и неориентированные)\n",
      "\n",
      "etypes_of_interest = {'e_legal_same_owner', 'e_gsk holder_leader_test',\n",
      "*e_legal_inn_same_owner', 'e_legal_inn_same_leader',\n",
      "*e_legal_same_leader'}\n",
      "\n",
      "company_edges_df = company_edges_df[company_edges_d¥.key.isin(etypes_of_interest)]\n",
      "\n",
      " \n",
      "\n",
      "# Загружаем данные об ОКВЭД\n",
      "okved_parts = ['okved_class_', 'okved_subclass', 'okved_group', 'okved_subgroup', 'окуей_хуре_']\n",
      "okved_data = pd.read_csv(CONFIG[ 'paths' ]['okved_data_save'],\n",
      "\n",
      "index_col=@,\n",
      "\n",
      "dtype={c: str for с in okved_parts})\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "# вспомогательные словари для маппинга ОКВЭДов в целые числа\n",
      "\n",
      "idx_to_okved = dict(zip(okved_data.index, окуед_Чата['окуеа' ]))\n",
      "\n",
      "okved_to_idx = dict(zip(okved_data['okved'], okved_data. index))\n",
      "\n",
      "section_to_idx = {section: idx for idx, section in enumerate(okved_datal “раздел” ] .ипзаие())}\n",
      "okved_to_section = okved_datal 'pasgen'].map(section_to_idx)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      " \n",
      "\n",
      "print('Start...')\n",
      "# создаем связи на основе информации о ГСК\n",
      "gc_edges = сгеате_вс_едвез(сопрапу_4+, сопрапу_едвез_4+, okved_to_idx)\n",
      "print('gc edges created...')\n",
      "displey(gc_edges.head(2))\n",
      "# создаем связи Ha основе Классификатора\n",
      "classifier_edges = create_classifier_edges(okved_data, okved_parts, gc_edges, okved_to_:\n",
      "print('classifier edges created...')\n",
      "display(classifier_edges.head(2))\n",
      "# создаем таблицу © данными об узлах\n",
      "ndata = create_ndata(company df, okved_to_idx, okved_data, gc_edges, okved_to_section)\n",
      "print('ndata created...')\n",
      "display(ndata.head(2))\n",
      "# создаем и сохраняем гетерограф\n",
      "g connected = create_dgl_graph(classifier_edges, gc_edges, ndata )\n",
      "with open(CONFIG[ 'paths' ]['okved_graph*], 'wb') as fp:\n",
      "pickle.dump(g connected, fp)\n",
      "print( 'graph created...*)\n",
      "print(“Done”)\n",
      "\n",
      " \n",
      "\n",
      "dx)\n",
      "\f",
      " \n",
      "\n",
      "/usr/bin/env python\n",
      "# coding: utf-8\n",
      "\n",
      "# In[1]:\n",
      "get_ipython().run_line_magic('load_ext', 'autoreload'\n",
      "get_ipython().run_line_magic('autoreload', '2°\n",
      "\n",
      "from imports import *\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "# In[2]:\n",
      "\n",
      "from models.samplers import NegativeHeteroGraphSampler\n",
      "from models.rgcn import RGCN\n",
      "from models.loss import HeteroMLPCELoss, HeteroDotCELoss, HeteroDMCELoss, HeteroLoss\n",
      "\n",
      "# In[3]:\n",
      "\f",
      "def create_dataloader(g: dgl.DGLHeteroGraph,\n",
      "n_layers: int = 2,\n",
      "neg_examples: int = 3,\n",
      "batch_size: int = 2500) -> dgl.dataloading.EdgeDataLoader:\n",
      "“'*CosgaeT даталоадер для обучения\n",
      "\n",
      "Args:\n",
      "Е (dgl.DGLHeteroGraph): граф с кодами ОКВЭД\n",
      "n_layers (int, optional): количество слоев в RGCN\n",
      "neg_examples (int, optional): количество отрицательных примеров на 1 положительный пример\n",
      "batch_size (int, optional): количество ребер в одном батче для обучения\n",
      "\n",
      "Returns:\n",
      "dgl.DGLHeteroGraph: даталоадер для обучения\n",
      "\n",
      "# для обучения берем все ребра от классификатора и 80% от ГСК\n",
      "train_eid_ dict = {('okved', 'classifier', 'окуед”): g.edges(etype=('okved', 'classifier', 'okved'), form='eid'),\n",
      "Cokved', 'вс', 'okved'): g.edges['gc'].datal 'train_mask'].nonzero().flatten()}\n",
      "# для валидации берем 20% от ГСК\n",
      "val_eid_dict = {('okved', 'classifier', 'okved'): в-еавез(етуре-('окуед', 'classifier','okved'), form=eid'),\n",
      "Cokved', 'вс', 'okved'): g.edges[ 'gc' ].datal 'test_mask* ].nonzero().flatten()}\n",
      "\n",
      " \n",
      "\n",
      "reverse_eids dict = {}\n",
      "for etype in g.canonical_etypes:\n",
      "Е = g.num_edges(etype) // 2\n",
      "reverse_eids dict[etype] = torch.cat([torch.arange(E, 2*Е), torch.arange(@, E)])\n",
      "\n",
      "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(n_layers)\n",
      "dataloader = dgl.dataloading.EdgeDataloader(g, train_eid dict, sampler,\n",
      "negative_sampler=NegativeHeteroGraphSampler(g,\n",
      "neg_examples-neg examples,\n",
      "\n",
      " \n",
      "\n",
      "reverse_eids = reverse_eids dict,\n",
      "batch_size-batch_size,\n",
      "shuffle=True,\n",
      "\n",
      "drop_last=False)\n",
      "\n",
      " \n",
      "\n",
      "return dataloader\n",
      "\f",
      "# In[4]:\n",
      "\n",
      "def create_model(in_feats: int, n_hidden: int,\n",
      "rout: int, n_layers: int,\n",
      "dropout_p: float, device: str) -> RGCN:\n",
      "“''CosgaeT модель для обучения\n",
      "\n",
      "Args:\n",
      "in_feats (int): количество атрибутов на узлах графа\n",
      "n_hidden (int): размерность скрытых слоев модели\n",
      "n_out (int): размерность выходного слоя модели\n",
      "n_layers (int): количество скрытых слоев модели\n",
      "dropout_p (float): вероятность дропаута\n",
      "device (str): устройство для обучения (на наших машинах - только cpu)\n",
      "\n",
      "Returns:\n",
      "dgl.DGLHeteroGraph: даталоадер для обучения\n",
      "\n",
      "model = RGCN(in_feats, n_hidden, n_out, n_layers, F.relu, dropout_p, g.etypes)\n",
      "model = model.to(device)\n",
      "return model\n",
      "\n",
      " \n",
      "\f",
      "def train(model: RGCN,\n",
      "criterion: HeteroLoss,\n",
      "dataloader: dgl.dataloading.EdgeDataLoader,\n",
      "g: dgl.DGLHeteroGraph,\n",
      "okved_embeddings_model_path: str,\n",
      "n_epochs: int = 100,\n",
      "log_every: int = 10,\n",
      "max_no_improvements: int = 5) -> tuple:\n",
      "\n",
      "**'Обучает модель\n",
      "\n",
      "'Args:\n",
      "model (RGCN): модель для обучения\n",
      "criterion (HeteroLoss): функция потерь\n",
      "dataloader (dgl.dataloading.EdgeDataLoader): даталоадер для обучения\n",
      "Е (dgl.DGLHeteroGraph): граф для обучения\n",
      "okved_embeddings_model_path (str): путь для сохранения модели\n",
      "п_еросвз (int): количество эпох для обучения\n",
      "log_every (int): war для отображения текущих результатов внутри эпохи\n",
      "max_no_improvements (int): максимальное кол-во эпох без улучшения качества\n",
      "\n",
      "Returns:\n",
      "tuple: обученная модель и результаты работы с последней эпохи\n",
      "\n",
      "nfeat = g.ndata[ 'features*]\n",
      "\n",
      "best_acc = 9\n",
      "\n",
      "no_improvements = 9\n",
      "\n",
      "optimizer = optim.Adami([{'params': model.parameters()}, {'params” :criterion.parameters()}], 1\n",
      "\n",
      "epochs_accs = []\n",
      "no_improvements = @\n",
      "\n",
      "for epoch in range(@, n_epochs):\n",
      "curr_epoch_labels = []\n",
      "curr_epoch_preds = []\n",
      "curr_epoch_probas = []\n",
      "\n",
      "for step, (input_nodes, pos_graph, neg graph, blocks) in tqdm(enumerate(dataloader)):\n",
      "\n",
      "batch_inputs = {'okved': nfeat[input_nodes].to(device).float()}\n",
      "pos_graph = pos_graph.to(device)\n",
      "neg_graph = neg_graph.to(device)\n",
      "\n",
      "blocks = [block.to(device) for block in blocks]\n",
      "batch_pred = model(blocks, batch_inputs)\n",
      "\n",
      "loss, score, label = criterion(batch_pred, pos_graph, neg_graph)\n",
      "optimizer. zero_grad()\n",
      "\n",
      "loss. backward()\n",
      "\n",
      "optimizer. step()\n",
      "\n",
      "predictions = (score.sigmoid() > @.5).long().flatten()\n",
      "\n",
      "curr_epoch_labels.extend(1abel.detach().numpy())\n",
      "curr_epoch_preds.extend(predictions.detach().numpy())\n",
      "\n",
      " \n",
      "\n",
      "001)\n",
      "\f",
      "curr_epoch_probas.extend(score.flatten().sigmoid().detach() .numpy())\n",
      "\n",
      "if step % log every\n",
      "acc = (predictions label).sum() / len(label)\n",
      "print(f*{epoch=:05d} | {step=:@5d} | loss={loss.item():.4F} | train_acc={acc.item():.4f}')\n",
      "curr_epoch_result = {'labels': curr_epoch_labels, “preds': curr_epoch_preds, 'ргобаз”: curr_epoch_probas}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "       \n",
      "\n",
      "е асс = (torch.LongTensor(curr_epoch_preds)\n",
      "epochs_accs.append(e_acc. item())\n",
      "print(F*Epoch={epoch:05d} train_acc: {e_acc.item():.4F}\n",
      "if e_acc - best_acc >= 1e-3:\n",
      "print(f*New best acc: {e_acc\n",
      "best_ace = е асс\n",
      "no_improvements = @\n",
      "torch.save(model, okved_embeddings_model_path)\n",
      "else:\n",
      "no_improvements += 1\n",
      "if no_improvements >= max_no_improvements:\n",
      "print(f*No improvements in {max_no_improvements} epochs')\n",
      "print(f*Best асс = {best_acc}')\n",
      "break\n",
      "\n",
      " \n",
      "\n",
      "torch.LongTensor(curr_epoch_labels)).sum() / len(curr_epoch_labels)\n",
      "\n",
      " \n",
      "   \n",
      "\n",
      "э\n",
      "\n",
      "model = torch. load(okved_embeddings_model_path)\n",
      "return model, curr_epoch_result\n",
      "\f",
      "def visualize _train_results(metrics: dict) -> Non\n",
      "Выводит ВОС-кривую и матрицу несоответствий\n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "metrics (dict): результаты работы ¢ последней эпохи\n",
      "\n",
      "def evaluate(g: dgl.DGLHeteroGraph, all_embeddings: torch.Tensor) -> None:\n",
      "*''Оценивает accuracy Ha тестовом множестве\n",
      "\n",
      "Args:\n",
      "Е (dgl.DGLHeteroGraph): граф для обучения\n",
      "all_embeddings (torch.Tensor): тензор эмбеддингов узлов.\n",
      "\n",
      " \n",
      "\n",
      "# In[6]:\n",
      "\n",
      "def visualize train_results(metrics: dict) -> Non\n",
      "Выводит ВОС-кривую и матрицу несоответствий\n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "metrics (dict): результаты работы ¢ последней эпохи\n",
      "\n",
      "fpr, tpr, thresholds = гос_сигуе (петг1сз[ '1а6е15'], metrics['probas'])\n",
      "plt.plot([@, 1], [9, 1], linestyl *, label='Random')\n",
      "plt.plot(fpr, tpr, linestyle='solid', label='Model')\n",
      "\n",
      "plt.xlabel ('FPR')\n",
      "\n",
      "plt.ylabel('TPR')\n",
      "\n",
      "plt.legend()\n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "cm = pd.DataFrame(confusion_matrix(metrics['labels'], metrics['preds']), index-['\n",
      "display(cm)\n",
      "\n",
      " \n",
      "\n",
      "» 'y-1'], columns=['y*=0',\n",
      "\n",
      " \n",
      "\n",
      "у^\n",
      "\f",
      "def evaluate(g: dgl.DGLHeteroGraph, all_embeddings: torch.Tensor) -> None:\n",
      "*''Оценивает accuracy Ha тестовом множестве\n",
      "\n",
      "Args:\n",
      "Е (dgl.DGLHeteroGraph): граф для обучения\n",
      "all_embeddings (torch.Tensor): тензор эмбеддингов узлов\n",
      "with g.local_scope():\n",
      "g-ndata[*h*] = all_embeddings\n",
      "g-apply_edges(criterion.apply_edges, etype='gc')\n",
      "logits = g.edges[*gc'].data['score']\n",
      "test_preds = (logits[g.edges['gc'].data['test_mask'], @].sigmoid() > @.5).long()\n",
      "test_true = torch.ones_like(test_preds)\n",
      "test_acc = (test_preds test_true).sum() / len(test_true)\n",
      "print(f'Test_acc: {test_acc.item():.4f}')\n",
      "\n",
      " \n",
      "\f",
      "# In[8]:\n",
      "\n",
      "# загружаем конфигурационный файл\n",
      "CONFIG = уап .зае_Лоа4(ореп( 'СОМЕТ6 .уат1', encoding='utf8'))\n",
      "\n",
      "# Загружаем данные об ОКВЭД\n",
      "okved_parts = ['okved_class_', 'okved_subclass', 'okved_group', 'окуей_зивгоир', 'okved_type_']\n",
      "okved_data = pd.read_csv(CONFIG[ 'paths' ]['okved_data_save'],\n",
      "index_col=@,\n",
      "dtype str for с in okved_parts})\n",
      "# вспомогательные словари для маппинга ОКВЭДОВ в целые числа\n",
      "idx_to_okved = okved_datal 'okved'].to_dict()\n",
      "section_to_idx = {s: idx for idx, $ in enumerate(okved_data['pasaen'].unique())}\n",
      "okved_to_section = okved_data[['okved', 'pasnen']].set_index('okved')['pasaen'].map(section_to_idx).to_dict()\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "# загружаем гетерограф\n",
      "with open(CONFIG[ 'paths' ]['okved_graph'], 'rb*) as fp:\n",
      "Е = pickle. load(fp)\n",
      "\n",
      "# создаем и обучаем модель\n",
      "\n",
      "device = 'cpu'\n",
      "\n",
      "assert device == 'cpu'\n",
      "\n",
      "dataloader = create_dataloader(g, n_layers=2)\n",
      "\n",
      "model = create_model(in_feats-g.ndata[ 'features' ].shape[1],\n",
      "n_hidden=128, n_out=32,\n",
      "n_layers=2, dropout_p=0.25,\n",
      "device=device)\n",
      "\n",
      "criterion = HeteroDMCELoss(emb_siz:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "jodel.n_classes, train_on_gi\n",
      "\n",
      " \n",
      "\n",
      "model, metrics = train(model, criterion, dataloader, g,\n",
      "CONFIG[ 'paths' ]['okved_embeddings_model'],\n",
      "n_epochs=5@, log every=10, max_no_improvements=5)\n",
      "# визуализируем результат\n",
      "visualize _train_results(metrics)\n",
      "all_embeddings = model.get_embeddings(g)['okved']\n",
      "evaluate(g, all_embeddings)\n",
      "\n",
      "# сохраняем эмбеддинги кодов ОКВЭД\n",
      "\n",
      "names = np.array([idx_to_okved[idx] for idx in g.ndata[ 'okved_idx'].numpy()])\n",
      "\n",
      "name_emb = dict(zip(names, all_embeddings.tolist()))\n",
      "\n",
      "with open(CONFIG[ 'paths' ]['okved_embeddings'], 'wb') as fp:\n",
      "pickle.dump(name_emb, fp)\n",
      "\f",
      "def draw_2d(embeddings_2d: пр.аггау,\n",
      "g: dgl.DGLHeteroGraph,\n",
      "okved_data: pd.DataFrame,\n",
      "okved_to_section: dict,\n",
      "idx_to_okved: dict,\n",
      "xlim: tuple = None,\n",
      "ylim: tuple = None,\n",
      "figsize: tuple = (15, 5),\n",
      "annotate: bool = False,\n",
      "name_len: int = 20,\n",
      "hide_spins: tuple = None,\n",
      "node_size: int = 5) -> None:\n",
      "Рисует проекции эмбеддингов на плоскости\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Args:\n",
      "embeddings_2d (np.array): массив эмбеддингов узлов\n",
      "g (dgl.DGLHeteroGraph): граф для обучения\n",
      "okved_data (pd.DataFrame): таблица с информацией об ОКВЭД\n",
      "okved_to_section (dict): маппинг код раздела ОКВЭД - номер кода\n",
      "idx_to_okved (dict): маппинг номер кода ОКВЭД - код\n",
      "xlim (tuple, optional): ограничения по оси x\n",
      "ylim (tuple, optional): ограничения по оси у\n",
      "figsize (tuple, optional): размер фигуры\n",
      "annotate (bool, optional): True, если нужно добавить подписи к точкам (лучше не применять на полном датасете)\n",
      "name_len (int, optional): максимальная длина названия кода\n",
      "hide_spins (tuple, optional): какие рамки скрывать\n",
      "node_size (int, optional): размер точки\n",
      "\n",
      " \n",
      "\n",
      "def cut(s: str, In: int = 10) -> str\n",
      "if len(s) >= 1\n",
      "return s[:1n-3] +\n",
      "\n",
      "return $\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "colors = np.array([okved_to_section[idx_to_okved[idx]] for idx in g.ndata[ 'okved_idx'].numpy()])\n",
      "names = np.array([idx_to_okved[idx] + ' ' + cut(okved_data.loc[idx, 'name'], name_len)\n",
      "for idx in g.ndata[ 'okved_idx'].numpy()])\n",
      "\n",
      "   \n",
      "\n",
      "fig, ax = plt.subplots(figsize-Figsize)\n",
      "\n",
      "if xlim:\n",
      "x_mask\n",
      "\n",
      "else:\n",
      "x_mask\n",
      "\n",
      "(embeddings 2d[:, 0] >= xlim[@]) & (embeddings 2d[:, 0] <= xlim[1])\n",
      "\n",
      " \n",
      "\n",
      "np.ones(1en(embeddings_2d)).astype(bool)\n",
      "\n",
      " \n",
      "\n",
      "if ylim:\n",
      "y_mask = (embeddings 2d[:, 1] >= ylim[@]) & (embeddings 2d[:, 1] <= ylim[1])\n",
      "else:\n",
      "y_mask = np.ones(1en(embeddings_2d)) .astype(bool)\n",
      "\n",
      "mask = x_mask & y mask\n",
      "embs = embeddings 2d[mask]\n",
      "\f",
      "embs = embeddings_2d[mask]\n",
      "colors = colors[mask]\n",
      "names = names[mask]\n",
      "\n",
      "ах.\n",
      "\n",
      "ах.\n",
      "\n",
      "if\n",
      "\n",
      "if\n",
      "\n",
      "if\n",
      "\n",
      "if\n",
      "\n",
      "-scatter(embs[:, 0], embs[:, 1], s-node_size, c-colors)\n",
      "ax.\n",
      "\n",
      " \n",
      "\n",
      "set_xlabel('$h_0(v)$')\n",
      "-set_ylabel('$h_1(v)$')\n",
      "\n",
      "annotate:\n",
      "\n",
      "for (x, y), txt in zip(embs, names):\n",
      "\n",
      "ax.text(x, y, txt, rotation=45)\n",
      "\n",
      "xlim:\n",
      "\n",
      "ax. set_xlim(*x1im)\n",
      "\n",
      "ylim:\n",
      "\n",
      "ax. set_ylim(*ylim)\n",
      "\n",
      "hide_spins is not None\n",
      "\n",
      "for spin in hide_spins:\n",
      "ax. spines[ spin]. set_visible(False)\n",
      "\n",
      " \n",
      "\n",
      "# In{11):\n",
      "\n",
      "embeddings_2d\n",
      "g-ndata['x']\n",
      "\n",
      "TSNE(2, random_state=43) .fit_transform(all_embeddings.detach())\n",
      "'torch. from_numpy(embeddings 2d[:, @])\n",
      "\n",
      " \n",
      "\n",
      "g-ndata['y'] = torch. from_numpy(embeddings_2d[:, 1])\n",
      "draw_2d(embeddings 2d, в, okved_data, okved_to_section, idx_to_okved, annotate-False)\n",
      "\f",
      "1 /usr/bin/env python\n",
      "# coding: utf-8\n",
      "\n",
      "# In{1]:\n",
      "\n",
      "get_ipython().run_line_magic(*load_ext', 'autoreload')\n",
      "get_ipython().run_line_magic('autoreload', '2°)\n",
      "\n",
      "from imports import *\n",
      "\n",
      "from encoders import TargetEncoder\n",
      "\n",
      "# In[2]:\n",
      "def get_company_okved_code(dataset_path: str, company_okved_dict_path: str):\n",
      "*''Строит словарь id компании - код ОКВЭД\n",
      "Args:\n",
      "dataset_path (str): путь к каталогу с pickle файлами с информацией о ГСК\n",
      "company _okved_dict_path (str): путь для сохранения словаря с ОКВЭД компании\n",
      "\n",
      "Returns:\n",
      "dic'\n",
      "\n",
      " \n",
      "\n",
      "словарь id компании - код ОКВЭД\n",
      "\n",
      "if isfile(company_okved_dict_path):\n",
      "return pickle. load(open(company_okved_dict_path, 'rb'))\n",
      "\n",
      "company _okved = {}\n",
      "with Path(dataset_path) as path:\n",
      "for file in tqdm(path.iterdir()):\n",
      "вс = pickle.load(open(file, 'rb'))\n",
      "for company in gc['nodes*\n",
      "company_okved[company[ 'id*]] = company[ 'okved_code']\n",
      "with open(company_okved_dict_path, 'wb') as fp:\n",
      "pickle.dump(company_okved, fp)\n",
      "return company_okved\n",
      "\n",
      "  \n",
      "\f",
      "def generate_financial_features(fin_features: pd.DataFrame) -> pd.DataFrame:\n",
      "““OnneTpyeT данные о финансовых показателях компании, оставляя только записи, относящиеся к компаниям,\n",
      "для которых есть информация за несколько лет и известен код ОКВЭД\n",
      "\n",
      "'Args:\n",
      "fin_features (pd.DataFrame): Таблица с финансовыми показателями компаний за несколько лет\n",
      "\n",
      "Returns:\n",
      "pd.DataFrame: Таблица < финансовыми показателями компаний, в которой нет столбцов с нулями\n",
      "и для каждой компании есть информация хотя бы за 2 года, отсортированная\n",
      "по ТО компании и году\n",
      "\n",
      "profit_col = 'р24004'\n",
      "fin_statement_cols = ['arango_id', 'balance _year', profit_col]\n",
      "\n",
      "for col in [1100, 1200, 1210, 1230, 1250, 1300, 1500, 1520, 1600, 1700, 2340, 2350]:\n",
      "fin_statement_cols.extend([f'p{col}3', f'p{col}4'])\n",
      "\n",
      "fin_features = fin_features[fin_statement_cols].copy()\n",
      "fin_features.drop_duplicates(subset=['arango_id', 'balance_year'], inplace-True)\n",
      "fin_features = fin_features[(fin_features[fin_statement_cols] == @).sum(axis:\n",
      "\n",
      "   \n",
      "\n",
      "company_years = fin_features[ 'arango_id'].value_counts()\n",
      "\n",
      "have_multiple_years = company_years[company_years>1].index\n",
      "\n",
      "fin_features = fin_features[(fin_features[ 'arango_id'].isin(have_multiple_years)) &\n",
      "(fin_features[ 'arango_id'].isin(company_okved)) ]\n",
      "\n",
      "fin_features.sort_values(['arango_id', 'balance _year'], inplace=True)\n",
      "\n",
      "fin_features.reset_index(drop=True, inplace=True)\n",
      "\n",
      "fin_features.insert(2, 'okved_code', fin_features['arango_id'].map(company_okved))\n",
      "\n",
      "fin_features[profit_col] = (fin_features[profit_col] >= @).astype(int)\n",
      "\n",
      "fin_features.rename(columns={profit_col: 'has_profit'}, inplace=True)\n",
      "\n",
      "fin_features.pop(*balance_year')\n",
      "\n",
      "return fin_features\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "def to_ptc_changes(df: pd.DataFrame) -> pd.DataFreme:\n",
      "'Считает процентные изменения финансовых показателей компаний относительно предыдущего года\n",
      "\n",
      "Args:\n",
      "4+ (pd.DataFrame): Таблица с финансовыми показателями компаний, отсортированная\n",
      "по ТО компании и году\n",
      "Returns:\n",
      "pd.DataFrame: Таблица < процентными изменениями финансовых показателей\n",
      "If = 92. сору()\n",
      "for col in df.columns[3:]:\n",
      "shifted_vals = df[col].shift().where(df['arango_id']\n",
      "df[col] = (df[col] - shifted_vals) / shifted_vals\n",
      "return d¥.dropna()\n",
      "\n",
      " \n",
      "\n",
      "df['arango_id'].shift())\n",
      "\f",
      "def create_dataset@(X_train: np.array, X_test: np.array, y train: np.array, y test: пр.аггау) -> tuple:\n",
      "*''Создает датасет без информации о коде ОКВЭД' **\n",
      "X_train.pop('okved_code')\n",
      "X_test.pop( 'okved_code')\n",
      "dset = X_trein, X_test, y train, y test\n",
      "return dset\n",
      "\n",
      "# In[5]:\n",
      "\n",
      "def create_dataset1(X_train: np.array, X_test: np.array, y train: np.array, y test:\n",
      "*''Создает датасет с кодами ОКВЭД, закодированными при помощи Target Encoding”\n",
      "okved_encoder = TargetEncoder().fit(X_train[ 'okved_code'], y_train)\n",
      "X_train[ 'okved_code'] = okved_encoder.transform(X_train[ 'okved_code'])\n",
      "X_test['okved_code'] = okved_encoder. transform(X_test[ 'okved_code'])\n",
      "dset = X_train, X_test, y train, y test\n",
      "return dset\n",
      "\n",
      "пр-аггау) -> tuple:\n",
      "\n",
      "  \n",
      "\n",
      "# In[6]:\n",
      "\n",
      "def create_dataset2(X_train: np.array, X_test: np.array,\n",
      "y_train: np.array, y_test: пр.аггау,\n",
      "okved_embeddings dict: dict) -> tuple:\n",
      "*''Создает датасет с кодами ОКВЭД, закодированными при умных эмбеддингов” **\n",
      "emb_dim = len(okved_embeddings_dict[1ist(okved_embeddings_dict.keys())[@]])\n",
      "new_cols = [F'okved_code_{i}' for i in range(emb_dim)]\n",
      "X_train[new_cols]\n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "X_train[ 'okved_code'].map(1ambda x: okved_embeddings_dict.get(x, np.zeros(emb_dim))).tolist()\n",
      "\n",
      "X_test[new_cols] = X_test['okved_code'].map(lambda x: okved_embeddings_dict.get(x, np.zeros(emb_dim))).tolist()\n",
      "\n",
      "X_train.pop(*okved_code')\n",
      "\n",
      "X_test.pop( 'okved_code')\n",
      "\n",
      "dset = X_train, X_test, y train, у test\n",
      "return dset\n",
      "\f",
      "def train_model(dset: tuple, n_epochs: int, print_each: int, Ir: float = 0.005) -> tuple:\n",
      "*''Обучает однослойную НС на данном датасете\n",
      "\n",
      "Args:\n",
      "dset (tuple): кортеж из 4 массивов: X train, X_test, y train, y test\n",
      "n_epochs (int): кол-во эпох для обучения\n",
      "print_each (int): шаг для вывода текущей информации во время обучения\n",
      "Ir (float, optional): скорость обучения\n",
      "\n",
      " \n",
      "\n",
      "Returns:\n",
      "tuple: обученная модель и метрики качества\n",
      "\n",
      "Х пап, Х лезь, y_train, y_test = [torch.from_numpy(df.values) for df in dset]\n",
      "X_train = X_train.float()\n",
      "\n",
      "X test = X_test.float()\n",
      "\n",
      "y_train = y_train.long().fletten()\n",
      "\n",
      "y_test = y_test.long().flatten()\n",
      "\n",
      "n_feats = X_train.shape[1]\n",
      "\n",
      "out_classes = 2\n",
      "\n",
      "model = nn.Sequential(nn.Linear(n_feats, 2))\n",
      "\n",
      "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(compute_class_weight( 'balanced',\n",
      "classes=np.array([®, 1]),\n",
      "b=y_train.numpy())))\n",
      "\n",
      " \n",
      "\n",
      "metrics = {\n",
      "“train_auc': [],\n",
      "“test_auc': []\n",
      "\n",
      "i\n",
      "\n",
      "optimizer = optim.Adam(model.parameters(), Ir=1r)\n",
      "for epoch in range(n_epochs:\n",
      "model. train()\n",
      "logits = model(X_train)\n",
      "loss = criterion(logits, y_train)\n",
      "loss. backward()\n",
      "optimizer. step()\n",
      "optimizer. zero_grad()\n",
      "\n",
      " \n",
      "\n",
      "train_logits = logits.detach()\n",
      "'train_auc = roc_auc_score(y train, train_logits[:, 1])\n",
      "\n",
      "logits_test = model(X_test).detach()\n",
      "test_auc = roc_auc_score(y test, logits_test[:, 1])\n",
      "\n",
      "if epoch % print_each == @ or epoch == n_epochs-1:\n",
      "print(f*{epoch=:@5d} | loss={loss.item():.4F} | train_auc={train_auc:.4f} *)\n",
      "\n",
      "metrics['train_auc'].append(train_auc)\n",
      "\n",
      "metrics['test_auc'].append(test_auc)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "return model, metrics\n",
      "\f",
      "# загружаем конфигурационный файл\n",
      "CONFIG = yaml.safe_load(open('CONFIG.yaml', encoding='utf8'))\n",
      "# получаем код ОКВЭД для каждой компании из датасета\n",
      "company_okved = get_company_okved_code(CONFIG[ 'paths ]['gc_augmented_save'], CONFIG[ 'paths*]['company_okved_dict'])\n",
      "with open(CONFIG[ 'paths*]['okved_embeddings'], 'rb') as fp:\n",
      "okved_embeddings_dict = pickle. load(fp)\n",
      "\n",
      "# создаем таблицу с финансовыми показателями\n",
      "\n",
      "if isfile(CONFIG[ 'paths' ]['profits_dataset']):\n",
      "fin_features_preproc = pd.read_csv(CONFIG[ 'paths' ][profits_dataset'])\n",
      "\n",
      "else:\n",
      "fin_features = pd.read_csv(CONFIG[ 'paths*]['fin_data'])\n",
      "fin_features_preproc = generate_financial_features(fin_features.copy())\n",
      "fin_features_preproc = to_ptc_changes(fin_features_preproc)\n",
      "fin_features_preproc.to_csv(CONFIG[ 'paths*]['profits_dataset'], index-False)\n",
      "\n",
      " \n",
      "\n",
      "display(fin_features_preproc.head(2))\n",
      "\n",
      "# создаем несколько версий датасета\n",
      "\n",
      "X = fin_features_preproc.drop(columns=['has_profit', “arango_id'])\n",
      "\n",
      "у = fin_features_preproc[*has_profit']\n",
      "\n",
      "X_train, X_test, y train, y test = train_test_split(X, у, test_size-@.2, random_state=@)\n",
      "\n",
      "dset® = create_dataset@(X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy())\n",
      "\n",
      "dset1 = create_dataseti(X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy())\n",
      "\n",
      "dset2 = create_dataset2(X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy(), okved_embeddings_dict)\n",
      "\n",
      " \n",
      "\n",
      "# In[9]:\n",
      "\n",
      "# обучаем 1 модель\n",
      "model@, metrics® = train_model(dset®, n_epochs-50@, print_each=50, 1^-0.05)\n",
      "\n",
      "# In[10]:\n",
      "\n",
      "# обучаем 2 модель\n",
      "поде11, metrics1 = train_model(dset1, n_epochs-50@, print_each=50, 1^-0.05)\n",
      "\n",
      "# In[11]:\n",
      "\n",
      "# обучаем 3 модель\n",
      "model2, metrics2 = train_model(dset2, n_epochs-50@, print_each=50, 1^-0.05)\n",
      "\f",
      "def plot_roc_auc(n_epochs: int = None, *model_metrics):\n",
      "\n",
      "“''Pucyer график значений ROC AUC за первые n_epocs эпох” '*\n",
      "\n",
      "descs = ['модель без ОКВЭД', 'модель с закодированным ОКВЭД', 'модель с умными эмбеддингами ОКВЭД' ]\n",
      "\n",
      "metric_name = 'ROC ДИС'\n",
      "\n",
      "fig, ах = plt.subplots(1, 1, figsize-(10, 5))\n",
      "\n",
      "for dset_idx, metrics in enumerate(model_metrics):\n",
      "ax.plot(metrics[f*test_auc'][:n_epochs], label=f'{descs[dset_idx]}')\n",
      "ax.set_title(f'{metric_name} Ha тестовом множестве')\n",
      "ах. set_xlabel('Epoch')\n",
      "ax. set_ylabel(f*{metric_name}')\n",
      "\n",
      "fig. legend(bbox_to_anchor=(8.9, 1.1))\n",
      "\n",
      "  \n",
      "\n",
      "# In[13]:\n",
      "\n",
      "# визуализируем результат\n",
      "plot_roc_auc(5@, metrics®, metrics1, metrics2)\n",
      "\f",
      "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
      "get_ipython().run_line_magic('autoreload', '2°)\n",
      "\n",
      "from imports import *\n",
      "\n",
      "from networkx.readwrite.json_graph import node_link_graph\n",
      "tqdm.pandas()\n",
      "\n",
      "pd. set_option('display.max_colwidth', None)\n",
      "\n",
      "# In[2]:\n",
      "\n",
      "from models.sage import SAGE\n",
      "from models.loss import DotCeLossWithOkvedDistances\n",
      "\n",
      "# In[3]:\n",
      "\n",
      "def get_pretrained_embedding tensor(idx_to_okved: dict, path_to_embeddings dict: str) -> torch.Tensor:\n",
      "Создает тензор с эмбеддингами ОКВЭД\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Args:\n",
      "idx_to_okved (dict): маппинг номер кода ОКВЭД - код\n",
      "path_to_embeddings dict: путь с словарю с эмбеддингами ОКВЭД\n",
      "\n",
      "Returns:\n",
      "\n",
      "torch.Tensor: тензор с эмбеддингами ОКВЭД\n",
      "\n",
      " \n",
      "\n",
      "with open(path_to_embeddings_dict, 'rb') as fp:\n",
      "embeddings _dict = pickle. load(fp)\n",
      "\n",
      "embedding _size = len(next(iter(embeddings_dict.values())))\n",
      "\n",
      "embedding tensor = torch.zeros(len(idx_to_okved), embedding size)\n",
      "for idx, okved in idx_to_okved.items():\n",
      "if okved in embeddings dict:\n",
      "embedding tensor[idx] = torch.FloatTensor(embeddings_dict[okved])\n",
      "return embedding tensor\n",
      "\f",
      "def create_dataset(dataset_dir: str, result_dir: str, etypes-None, **kwargs):\n",
      "*''Обрабатывает все файлы из каталога и подгатавливает датасет\n",
      "\n",
      "Args:\n",
      "dataset_dir (str): путь к каталогу ¢ pickle файлами с информацией о ГСК\n",
      "result_dir (str): путь для сохранения датасета\n",
      "etypes (list, optional): список типов узлов для добавления в датасет\n",
      "\n",
      "Returns:\n",
      "list: датасет для решения задачи классификации ребер как находящихся внутри ГСК или ведущих вовне ГСК\n",
      "dataset = []\n",
      "for fname in tqdm(list(Path(dataset_dir).iterdir())):\n",
      "with open(fname, 'rb') as fp:\n",
      "data = pickle. load(fp)\n",
      "node_membership = {}\n",
      "\n",
      "*unknown' ]\n",
      "\n",
      " \n",
      "\n",
      "data['nodes'] = [node for node in data['nodes*] if node[ 'okved_code']\n",
      "\n",
      "# атрибуты узлов - номер кода ОКВЭД, финансовые показатели, ID компании\n",
      "# оставляем только членов ГСК\n",
      "for node in Чата[ 'подез']:\n",
      "node[ 'okved_code'] = okved_to_idx[node[ 'okved_code']]\n",
      "node['profit'] = np.array([node[k] for К in [*p21103', *p21104']])\n",
      "node 'company _id*] = node[ '14' ]\n",
      "node[*gc_root*] = data[ 'graph' ]['root']\n",
      "node_membership[node[ 'id' ]] = node[ “входит_в_ГСК']\n",
      "\n",
      "# убираем ребра для удаленных узлов\n",
      "дата['14пк5'] = [link for link in data['links*] if (link['source'] in node_membership and\n",
      "Link['target'] in node_membership)]\n",
      "\n",
      "for edge in data 'links*]:\n",
      "edge['gc_root'] = data[ 'graph' ][ 'гост' ]\n",
      "# 1 если оба узла входят в ГСК, 9 - если хотя бы один не входит\n",
      "edge['label'] = int(node_membership[edge[ 'зоигсе']] and node_membership[edge[ 'target' ]])\n",
      "\n",
      "# фильтруем типы связей\n",
      "if etypes is not None:\n",
      "data['links'] = [edge for edge in data['links'] if edge[*key'] in etypes]\n",
      "\n",
      "if not data['links*] or not data[ 'nodes' ]:\n",
      "continue\n",
      "\f",
      "H = node_link_graph(data, directed=True, multigraph=False)\n",
      "# убираем изолированные узлы, появившиеся в результате удаления других узлов или связей\n",
      "H.remove_nodes_from(list(nx. isolates(H)))\n",
      "if 'root' not in H.graph and 'TCK_root' in data:\n",
      "H.graph[ 'root'] = Чата[ 'ГСК_гоот' ]\n",
      "Е = dgl.from_networkx(H,\n",
      "node_attrs=['profit', 'okved_code', 'company id', 'входит_в_ГСК', 'вс _гоот'],\n",
      "edge_attrs-['label'])\n",
      "in_deg = g.in_degrees().view(-1, 1)\n",
      "out_deg = g.out_degrees().view(-1, 1)\n",
      "raw_profit = g.ndata['profit']\n",
      "# вместо абсолютных значений берем знак raw_profit\n",
      "profit = torch.where(raw_profit < @, 0,\n",
      "torch.where(raw_profit == 0, 1, 2))\n",
      "в.пдата['еатз'] = torch.cat([profit.view(-1, 2),\n",
      "in_deg,\n",
      "out_deg], dim=1)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "# 80% на обучение\n",
      "\n",
      "в.едата[ 'газп_пазк'] = torch.zeros(g.num_edges(), dtype=torch. bool) .bernoulli(®.8)\n",
      "g-gc_root = data[ 'graph' ]['root']\n",
      "\n",
      "dataset. append(g)\n",
      "\n",
      "# фильтруем слишком маленькие или слишком большие графы\n",
      "dataset = [g for g in dataset if 5 <= g.num_nodes() <= 50]\n",
      "random, shuffle(dataset)\n",
      "\n",
      "with open(result_dir, 'wb') as fp:\n",
      "pickle.dump(dateset, fp)\n",
      "\n",
      "return dataset\n",
      "\f",
      "def train(dataset: list,\n",
      "company embeddings _model_path: str,\n",
      "embedding tensor: torch.Tensor,\n",
      "int =32, n_out: int = 8,\n",
      "max_no_improvements: int = 5,\n",
      "print_each: int = 5,\n",
      "п еросвз: int = 10\n",
      "*''Обучает модель для решения задачи классификации ребер как находящихся внутри ГСК или ведущих вовне ГСК\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "Args:\n",
      "dataset (list[dgl.DGLGraph]): датасет для обучения модели\n",
      "company _embeddings_model_path (str): путь для сохранения модели\n",
      "n_hidden (int): размерность скрытых слоев модели\n",
      "n_out (int): размерность выходного слоя модели\n",
      "print_each (int): war для отображения текущих результатов внутри эпохи\n",
      "max_no_improvements (int): максимальное кол-во эпох без улучшения качества\n",
      "\n",
      "Returns:\n",
      "SAGE: обученная модель для классификации ребер Kak находящихся внутри ГСК или ведущих вовне ГСК\n",
      "n_features = dataset[0].ndata['feats'].shape[1] + embedding tensor.shape[1]\n",
      "model = SAGE(n_features, n_hidden,\n",
      "rout, n_layers=2,\n",
      "activation-F.relu, dropout-0.15,\n",
      "freeze-True,\n",
      "embedding _tensor-embedding_tensor)\n",
      "optimizer = optim.Adam(model.parameters(), 1^-0.005)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "all_labels = torch.cat([g.edata[ label] for g in dataset])\n",
      "\n",
      "neg_counts, pos_counts = all_labels.bincount()\n",
      "\n",
      "criterion = DotCeLossiWithOkvedDistances(model.embeddings, pos_weight=pos_counts/neg_counts, okved_:\n",
      "best_acc = 9\n",
      "\n",
      "no_improvements = 9\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "mpact=@.5)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "dataloader = dgl.dataloading.GraphDataLoader(dataset, batch_size=256, shuffle=True, drop_last-False)\n",
      "\f",
      "for epoch in range(n_epochs):\n",
      "train_correct, test_correct, train_total, test_total = 0, 0, 0, 9\n",
      "epoch_test_preds = []\n",
      "epoch_test_labels = []\n",
      "for g in tqdm(dataloader):\n",
      "node_features = g.ndate['feats'].float()\n",
      "okveds = g.ndata[ 'okved_code'].1ong()\n",
      "edge_label = в.едата[ '1а6е1']\n",
      "'train_mask = g.edatal 'train_mask']\n",
      "\n",
      "preds = model(g, node_features, okveds)\n",
      "\n",
      "loss, score, labels = criterion(g, preds, okveds, edge_label, train_mask)\n",
      "loss. backward()\n",
      "\n",
      "optimizer. step()\n",
      "\n",
      "optimizer. zero_grad()\n",
      "\n",
      "train_predictions = (score.sigmoid() > 0.5)\n",
      "train_correct += (train_predictions == labels).sum()\n",
      "train_total += len(lebels)\n",
      "\n",
      "_, test_score, test_labels = criterion(g, preds, okveds, edge_label, ~train_mask)\n",
      "test_predictions = (test_score.sigmoid() > 0.5)\n",
      "\n",
      "test_correct += (test_predictions == test_labels).sum()\n",
      "\n",
      "test_total += len(test_lebels)\n",
      "\n",
      "   \n",
      "\n",
      "epoch_test_preds.extend(test_predictions.tolist())\n",
      "epoch_test_labels.extend(test_labels.tolist())\n",
      "\n",
      "test_acc = test_correct / test_total\n",
      "train_ace = train_correct / train_total\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "if epoch % print_each == @ or epoch == n_epochs -\n",
      "print(f*{epoch=:@5d} | loss={loss.item():.4F} | train_acc={train_acc.item():.4f} | test_acc={test_acc.item():.4F}')\n",
      "if test_acc - best_acc >= 1e-3:\n",
      "print(f*New best acc: {test_acc,\n",
      "best_acc = test_acc\n",
      "no_improvements = @\n",
      "torch.save(model, company_embeddings_model_path)\n",
      "else:\n",
      "no_improvements += 1\n",
      "if no_improvements >= max_no_improvements:\n",
      "print(f*No improvements in {max_no_improvements} epochs')\n",
      "print(f*Best acc = {best_acc}')\n",
      "break\n",
      "model = torch. load(company_embeddings_model_path)\n",
      "return model\n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      ")\n",
      "\f",
      "def get_company_graph_with_embeddings(dataset: list, model: SAGE) -> dgl.DGLGraph:\n",
      "“''Crpowt граф, содержащий данные о всех компаниях датасета\n",
      "\n",
      "Args:\n",
      "dataset (list[dgl.DGLGraph]): датасет для обучения модели\n",
      "model: обученная модель для классификации ребер как находящихся внутри ГСК или ведущих вовне ГСК\n",
      "\n",
      "Returns:\n",
      "граф, содержащий данные о всех компаниях датасета с эмбеддингами узлов на атрибутах\n",
      "\n",
      "поде1 .еуа1()\n",
      "\n",
      "batch = dgl.batch(dataset)\n",
      "\n",
      "batch_features = batch.ndatal 'feats' ].float()\n",
      "\n",
      "batch_okveds = batch.ndata[ 'okved_code'].1long()\n",
      "\n",
      "all_embeddings = model(batch, batch_features, batch_okveds) .detach()\n",
      "\n",
      "batch.ndatal 'embeddings*] = all_embeddings\n",
      "\n",
      "return batch\n",
      "\f",
      "# загружаем конфигурационный файл\n",
      "CONFIG = yaml.safe_load(open(*CONFIG.yaml', encoding='utf8'))\n",
      "# получаем код ОКВЭД для каждой компании из датасета\n",
      "okved_parts = ['окчед <1азз_', 'окуед_зибс1азз', 'okved_group', 'okved_subgroup', 'okved_type_\n",
      "okved_data = pd.read_csv(CONFIG[ 'paths' ] ['окуед_дата_зауе'],\n",
      "index_col=@,\n",
      "dtype={c: str for с in okved_parts})\n",
      "# вспомогательные словари для маппинга ОКВЭДОВ в целые числа\n",
      "idx_to_okved = okved_data[ 'okved'].to_dict()\n",
      "okved_to_idx = {v:k for К, v in idx _to_okved.items()}\n",
      "section_to_idx = {s: idx for idx, $ in enumerate(okved_data['pasaen'].unique())}\n",
      "okved_to_section = okved_data[['okved', 'pasnen']].set_index('okved')['pasaen'].map(section_to_idx).to_dict()\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "# создаем тензор с эмбеддингами\n",
      "embedding tensor = get_pretrained_embedding tensor(idx_to_okved, CONFIG[ 'paths' ][ 'okved_embeddings'])\n",
      "# загружаем датасет\n",
      "inout_path = CONFIG[ 'paths' ]['inout_dataset']\n",
      "gc_path = CONFIG[ 'paths' ]['gc_augmented_save']\n",
      "if isfile(inout_path):\n",
      "with open(inout_path, 'rb') as fp:\n",
      "dataset = pickle. load(fp)\n",
      "else:\n",
      "dataset = create_dataset(gc_path, inout_path)\n",
      "# обучаем модель\n",
      "model = train(dataset,\n",
      "company_embeddings_model_path=CONFIG[ 'paths' ][*company_embeddings_model'],\n",
      "embedding _tensor=embedding tensor,\n",
      "n_hidden=32,\n",
      "n_out=8,\n",
      "max_no_improvements=5,\n",
      "print_each-5,\n",
      "n_epochs= 15)\n",
      "\n",
      "# собираем один большой граф, где есть все компании и их эмбеддинги\n",
      "batch = get_company_graph_with_embeddings(dataset, model)\n",
      "\n",
      "# In[8]:\n",
      "\n",
      "def print_gc_and_companies(dataset: list, lower: int, upper: int):\n",
      "Выводит на экран крупные ГСК и компании, которые в них входят\n",
      "\n",
      " \n",
      "\n",
      "gc_graph = {g.gc_root: g for g in dataset}\n",
      "\n",
      "large_gcs = [g.gc_root for g in dataset if в.пдата[ 'входит_в_ГСК'].6001().зип() >= 15][lower: upper]\n",
      "\n",
      "for gc_id in large_ge\n",
      "g = gc_graph[gc_id]\n",
      "companies = g.ndata[ 'company_id' ][g.ndata[ 'exoaut_s_[CK'].bool()].tolist()\n",
      "print(f*ID ГСК: {gc_id} Компании: {companies[:10]}')\n",
      "\n",
      " \n",
      "\f",
      "def print_gc_ranked_table(okved_data: pd.DataFrame,\n",
      "gc_id: int,\n",
      "node_id: int,\n",
      "model: SAGE,\n",
      "max_k: int = 20) -> None:\n",
      "“''Crpowr и выводит на экран таблицу близости компаний внутри ГСК gc_id относительно целевой компании поде_14\n",
      "\n",
      " \n",
      "\n",
      "Arg\n",
      "\n",
      " \n",
      "\n",
      "okved_data (pd.DataFrame): таблица с информацией об ОКВЭД\n",
      "\n",
      "gc_id (int): ID целевой ГСК\n",
      "\n",
      "node_id (int): ID целевого узла\n",
      "\n",
      "model (SAGE): обученная модель для классификации ребер как находящихся внутри ГСК или ведущих вовне ГСК\n",
      "max_k (int, optional): максимальное кол-во строк для вывода\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "def print_same_okved_ranked_table(batch: dgl.DGLGraph,\n",
      "okved_data: pd.DataFrame,\n",
      "idx_to_okved: dict,\n",
      "int,\n",
      "id: int,\n",
      "model: SAGE,\n",
      "max_k: int = 20):\n",
      "“''CrpowT и выводит на экран таблицу близости компаний, имеющих одинаковый код ОКВЭД,\n",
      "относительно целевой компании node_id\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Args:\n",
      "batch (dgl.DGLGraph): граф, содержащий данные о всех компаниях датасета с эмбеддингами узлов на атрибутах\n",
      "okved_data (pd.DataFrame): таблица с информацией об ОКВЭД\n",
      "idx_to_okved (dict): маппинг номер кода ОКВЭД - код\n",
      "gc_id (int): ID целевой ГСК\n",
      "node_id (int): ID целевого узла\n",
      "model (SAGE): обученная модель для классификации ребер как находящихся внутри ГСК или ведущих вовне ГСК\n",
      "max_k (int, optional): максимальное кол-во строк для вывода\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "def print_gc_ranked_table(okved_data: pd.DataFrame,\n",
      "gc_id: int,\n",
      "node_id: int,\n",
      "model: SAGE,\n",
      "max_k: int = 20) -> None:\n",
      "Строит и выводит Ha экран таблицу близости компаний внутри ГСК gc_id относительно целевой компании поде_14\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "'Args:\n",
      "okved_data (pd.DataFrame): таблица с информацией об ОКВЭД\n",
      "gc_id (int): ID целевой ГСК\n",
      "node_id (int): ID целевого узла\n",
      "model (SAGE): обученная модель для классификации ребер как находящихся внутри ГСК или ведущих вовне ГСК\n",
      "max_k (int, optional): максимальное кол-во строк для вывода\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "gc_graph = {g.gc_root: g for g in dataset}\n",
      "assert gc_id in gc_graph\n",
      "g = gc_graph[gc_id]\n",
      "with g-local_scope():\n",
      "# получаем эмбеддинги для всех узлов\n",
      "gc_embeddings = model(g, в.паата[ '+еатс'].#1оа(), g.ndatal'okved_code'].long())\n",
      "g-ndata[ 'embeddings' ] = gc_embeddings\n",
      "# берем подграф на компаниях, входящих в ГСК\n",
      "in_gc_mask = в.паата[входит_в_ГСК'] .6001()\n",
      "h = g.subgraph(in_gc_mask)\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "# считаем расстояния между каждой парой узлов (на основе эмбеддингов)\n",
      "gc_embeddings = В.паата[ 'embeddings' ].detach()\n",
      "\n",
      "pu_dists = torch. Tensor(pairwise_distances(gc_embeddings, metric-'euclidean'))\n",
      "# node_graph_idx - номер целевой компании в подграфе\n",
      "\n",
      "node_graph_idx = (h.ndata['company_id'] == node_id).nonzero().item()\n",
      "g.ndata.pop( 'embeddings' )\n",
      "\n",
      " \n",
      "\n",
      "# берем ближайшие компании\n",
      "\n",
      "dist_from_source = pw_dists[node_graph_idx]\n",
      "\n",
      "К = min(max_k, len(pw_dists))\n",
      "\n",
      "values, indices = dist_from_source.topk(k, largest-False)\n",
      "\n",
      "comp_id = h.ndata[ 'company id' ][indices]\n",
      "\n",
      "# ... их коды ОКВЭД\n",
      "\n",
      "okved_id = [idx_to_okved[i] for i in h.ndatal 'okved_code'][indices].tolist()]\n",
      "okved_name = okved_data.set_index('okved')[ 'name' ][okved_id]\n",
      "\n",
      " \n",
      "\n",
      "ре4пе('Целевой узел: ', node_id, 'ГСК: *, gc_id)\n",
      "df = ра.ОатаЕгате({'Ранг от целевого узла': np.arange(k),\n",
      "'Расстояние по эмбеддингам': values.tolist(),\n",
      "'ТО компании': comp_id,\n",
      "'Код ОКВЭД': okved_id,\n",
      "'Код ОКВЭД с расшифровкой”: okved_name,\n",
      "'Входит в ГСК': В.пдата[ 'входит _в_ГСК' ][indices].tolist()})\n",
      "ЗЕ['Ранг от целевого узла'] = пр.агапве(1еп(4+))\n",
      "15р1ау(9+)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "def print_same_okved_ranked_table(batch: dgl.DGLGraph,\n",
      "okved_data: pd.DataFrame,\n",
      "idx_to_okved: dict,\n",
      "gc_id: int,\n",
      "node_id: int,\n",
      "model: SAGE,\n",
      "max_k: int = 20):\n",
      "\n",
      "“''CrpowT и выводит на экран таблицу близости компаний, имеющих одинаковый код ОКВЭД,\n",
      "относительно целевой компании node_id\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Args:\n",
      "batch (dgl.DGLGraph): граф, содержащий данные о всех компаниях датасета с эмбеддингами узлов на атрибутах\n",
      "okved_data (pd.DataFrame): таблица с информацией об ОКВЭД\n",
      "idx_to_okved (dict): маппинг номер кода ОКВЭД - код\n",
      "gc_id (int): ID целевой ГСК\n",
      "node_id (int): ID целевого узла\n",
      "model (SAGE): обученная модель для классификации ребер как находящихся внутри ГСК или ведущих вовне ГСК\n",
      "max_k (int, optional): максимальное кол-во строк для вывода\n",
      "\n",
      " \n",
      "\n",
      "gc_graph = {g.gc_root: g for g in dataset}\n",
      "\n",
      "assert gc_id in gc_graph\n",
      "\n",
      "target_g = gc_graph[gc_id]\n",
      "\n",
      "okveds = target_g.ndatal 'okved_code'].1long()\n",
      "\n",
      "# находим номер ОКВЭД целевой компании\n",
      "source_okved_id = okveds[(target_g.ndata[ 'сопрапу_14']\n",
      "# отбираем все компании < тем же кодом\n",
      "same_okved_mask = batch.ndata[ 'окуед_соде' ]=зоигсе_окуед_19\n",
      "companies = batch.ndata[ ' сопрапу_14' ][same_okved_mask].tolist()\n",
      "gc_ids = batch.ndata['gc_root* ][Same_okved_mask].tolist()\n",
      "\n",
      " \n",
      "\n",
      "node_id) .nonzero().item()]\n",
      "\n",
      " \n",
      "\n",
      "embs = {}\n",
      "# итерируемся по компаниям с тем же ОКВЭД\n",
      "for i, c in zip(gc_ids, companies):\n",
      "# берем ГСК очередной компании и получаем эмбеддинги для всех узлов\n",
      "Е = gc_graph[i]\n",
      "gc_embeddings = model(g, g.ndata['feats'].float(), g.ndata[ 'okved_code'].1ong())\n",
      "# забираем эмбеддинг интересующего нас узла и кладем в словарь\n",
      "node_graph_idx = (в.пдата[' сопрапу_14'] == c).nonzero().item()\n",
      "embs[(i, c)] = gc_embeddings[node_graph_idx]\n",
      "\n",
      "res = []\n",
      "for (gid, cid), emb in embs.items():\n",
      "res.append({'ID ГСК': gid,\n",
      "“ID Компании': cid,\n",
      "'Расстояние от целевой вершины': torch.dist(embs[(gc_id, node_id)], embs[(gid, cid)]).item()\n",
      "\n",
      "У\n",
      "\n",
      "# ID ГСК читать как: узел из ГСК или около ГСК\n",
      "\n",
      "ре4пе('Целевой узел: ', node_id, ° ГСК: ', gc_id, 'код ОКВЭД: ', idx_to_okved[source_okved_id.item()])\n",
      "res = ра.ОатаЕгапе (гез) .зогт_уа1иез( 'Расстояние от целевой вершины') .гезет_1пдех(агор-Тгие) .head(max_k)\n",
      "display(res)\n",
      "\n",
      " \n",
      "\f",
      "# In[11]:\n",
      "\n",
      "print_gc_and_companies(dataset, @, 10)\n",
      "\n",
      "# In[12]:\n",
      "\n",
      "# пример ранжирования компаний внутри ГСК\n",
      "print_gc_ranked_table(okved_data, gc_id=4755870, поде_\n",
      "\n",
      " \n",
      "\n",
      "7887927, model=model, max_}\n",
      "\n",
      " \n",
      "\n",
      "# In[13]:\n",
      "\n",
      "# пример ранжирования компаний < одним ОКВЭД\n",
      "print_same_okved_ranked_table(batch, okved_data, idx_to_okved, gc_id=4755870, node_id=7887927, mode:\n",
      "\n",
      " \n",
      "\n",
      "jodel, max_}\n",
      "\n",
      " \n",
      "\f",
      "directions:\n",
      "directed:\n",
      "e_gsk_holder: @\n",
      "e_legal_inn_affilated_legal: 1\n",
      "e_legal_inn_owns_legal: 2\n",
      "e_legal_inn_transaction: 3\n",
      "e_gsk_firm_holder_test: 4\n",
      "e_legal_inn_demo_transactions: 5\n",
      "undirected:\n",
      "e_gsk_holder_leader_test: 6\n",
      "e_legal_inn_same_leader: 7\n",
      "e_legal_inn_same_owner: 8\n",
      "e_legal_legal_miismi: 9\n",
      "e_legal_same_leader: 10\n",
      "e_legal_same_owner: 11\n",
      "constants:\n",
      "max_date: 2021-12-31\n",
      "max_company age: 20\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "fill_value:\n",
      "int: @\n",
      "object: unknown\n",
      "float: 0.0\n",
      "financial:\n",
      "\n",
      "[1100, 1130, 1150, 1160, 1170, 1190, 1200, 1210,\n",
      "1220, 1230, 1250, 1300, 1350, 1360, 1370, 1400,\n",
      "1410, 1450, 1500, 1510, 1520, 1530, 1600, 1700,\n",
      "2100, 2110, 2120, 2200, 2210, 2220, 2300 ‚2320,\n",
      "2330, 2340, 2350, 2400, 2410]\n",
      "\n",
      "paths:\n",
      "fin_data: './data/additional/fin_arango_gsk.csv'\n",
      "bankrupt_data: *./data/additional/bankrupt_arango_gsk.csv'\n",
      "gc_data: *./data/TCk_v3'\n",
      "gc_augmented_save: *./data/TCK_v3_augmented/pickle*\n",
      "okved_doc: './data/additional/foxyment предоставлен КонсультантПлюс. docx'\n",
      "okved_data_save: './data/additional/okved_data.csv'\n",
      "gc_augmented_node_feature /data/T'CK_v3_augmented/node_features_with_gc.csv'\n",
      "gc_augmented_edge features: *./data/TCK_v3_augmented/edge_features.csv'\n",
      "okved_graph: *./data/additional/okved_graph.pickle*\n",
      "okved_embeddings_model: *./models/checkpoints/okved_embedding_model.pth*\n",
      "okved_embeddings: *./data/embeddings/okved_embeddings.pickle*\n",
      "company_okved_dict: './data/additional/company_okved_dict.pickle*\n",
      "profits dataset: './data/additional/profits_dataset.csv'\n",
      "inout_dataset: *./data/additional/company_inout_graphs.pickle'\n",
      "company_embeddings_model: *./models/checkpoints/company_embedding_model.pth*\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "from __future__ import annotations\n",
      "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
      "import pandas as pd\n",
      "import numpy as пр\n",
      "\n",
      "class OrdinalEncoder(object):\n",
      "def _ init__(self, columns: list):\n",
      "self.columns = columns\n",
      "self.encoders = {column: LabelEncoder() for column in self.columns}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "def fit(self, data: pd.DataFrame) -> OrdinalEncoder:\n",
      "transformed = pd.DataFrame()\n",
      "for column in self.columns:\n",
      "if column not in data:\n",
      "print(f'Warning: {column} not in data')\n",
      "continue\n",
      "transformed[column] = self.encoders[ column] .fit_transform(data[column])\n",
      "return self\n",
      "\n",
      " \n",
      "\n",
      "def transform(self, data: pd.DataFrame) -> dict:\n",
      "transformed = pd.DataFrame()\n",
      "for column in self.columns:\n",
      "try:\n",
      "transformed[ column]\n",
      "except TypeError as e:\n",
      "raise TypeError(f'{e} while processing {column}')\n",
      "transformed.index = data.index\n",
      "return transformed\n",
      "\n",
      " \n",
      "\n",
      "self.encoders[ column] .transform(data[column])\n",
      "\n",
      "def fit_transform(self, data):\n",
      "self. fit(data)\n",
      "return self.transform(data)\n",
      "\n",
      "@property\n",
      "def state_dict(self):\n",
      "state_dict = {}\n",
      "\n",
      " \n",
      "\n",
      "for column in self.columns:\n",
      "classes = self.encoders[column].classes_\n",
      "value_idx_mapping = {class_: idx for idx, class_ in enumerate(classes)}\n",
      "state_dict[column] = value_idx_mapping\n",
      "\n",
      "return state_dict\n",
      "\n",
      " \n",
      "\f",
      "class TargetEncoder(object):\n",
      "def _init_(self, min_samples_leaf=1, smoothing=1, noise_level=\n",
      "self.min_samples_leaf = min_samples_leaf\n",
      "self.smoothing = smoothing\n",
      "self.noise_level = noise_level\n",
      "\n",
      " \n",
      "\n",
      "def fit(self, x, у):\n",
      "temp = pd.concat([x, y], axis=1)\n",
      "averages = temp.groupby(x.name)[y.name].agg(['mean', 'count' ])\n",
      "smoothing = 1 / (1 + np.exp(-(averages['count'] - self.min_samples_leaf) / self.smoothing))\n",
      "self.prion = y.mean()\n",
      "averages[y.name] = self.prior * (1 - smoothing) + averages['mean'] * smoothing\n",
      "averages.drop(['mean', 'count'], axis=1, inplace=True)\n",
      "self.averages = averages\n",
      "self.y_name = y.name\n",
      "return self\n",
      "\n",
      "def transform(self, x: pd.Series) -> pd.Series:\n",
      "¥t_x = pd.merge(x.to_frame(x.name),\n",
      "self.averages.reset_index().rename(columns={'index': self.y_name, self.y name: 'average'}),\n",
      "on=x.name, how='left')['average'].rename(x.name + '_mean').fillna(self.prior)\n",
      "ft_x.index = x. index\n",
      "\n",
      " \n",
      "\n",
      "return self.add_noise(ft_x)\n",
      "\n",
      "def fit_transform(self, data)\n",
      "self. fit(data)\n",
      "return self.transform(data)\n",
      "\n",
      "def add_noise(self, $):\n",
      "return $ * (1 + self.noise_level * np.random.randint(len(s)))\n",
      "\n",
      "def target_encode(x, у, x test, min_samples_leaf=1, smoothing=1, noise_level=0):\n",
      "\n",
      "ft_x_test = pd.merge(x_test.to_frame(x_test.name),\n",
      "averages .reset_index().rename(columns={'index': y.name, y.name: 'average'}),\n",
      "on=x_test.name, how='left')['average'].rename(x_test.name + '_mean').fillna(prior)\n",
      "ft_x_test.index = x_test.index\n",
      "return add_noise(ft_x, noise_level), add_noise(ft_x_test, noise_level)\n",
      "\n",
      "   \n",
      "\f",
      "from copy import deepcopy\n",
      "import random\n",
      "import os\n",
      "from os.path import isfile\n",
      "from pathlib import Path\n",
      "import pickle\n",
      "from tqdm.notebook import tqdm\n",
      "\n",
      "from IPython.display import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import\n",
      "\n",
      "import numpy аз np\n",
      "import pandas as pd\n",
      "\n",
      "import\n",
      "\n",
      "from\n",
      "from\n",
      "from\n",
      "from\n",
      "from\n",
      "from\n",
      "from\n",
      "from\n",
      "\n",
      "seaborn as sns\n",
      "\n",
      "networkx as nx\n",
      "\n",
      "sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "sklearn.metrics\n",
      "sklearn.metrics\n",
      "sklearn.metrics\n",
      "sklearn.metrics\n",
      "\n",
      "import pairwise distances, confusion_matrix\n",
      "import roc_curve\n",
      "\n",
      "import roc_auc_score, classification_report\n",
      "import mean_squared_error, f1_score\n",
      "\n",
      "sklearn.model_selection import train_test_split\n",
      "sklearn.utils.class_weight import compute_class_weight\n",
      "tqdm.notebook import tqdm\n",
      "\n",
      "import yam]\n",
      "\n",
      "import torch\n",
      "\n",
      "import\n",
      "import\n",
      "import\n",
      "\n",
      "import dg]\n",
      "import dgl.nn as gnn\n",
      "\n",
      "import\n",
      "\n",
      "torch.manual_seed(®)\n",
      "random.seed()\n",
      "\n",
      "np. random. зее4(0)\n",
      "dgl.seed(2)\n",
      "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE*\n",
      "\n",
      "torch.nn as пп\n",
      "torch.nn. functional аз Е\n",
      "torch.optim as optim\n",
      "\n",
      "dgl.function as fn\n",
      "\f",
      "import dgl.function as fn\n",
      "import torch.nn as nn\n",
      "\n",
      "import torch\n",
      "\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class Нетего! 055 (пп.Моди1е) :\n",
      "*”'Базовый класс для функции потерь для предсказания связей на гетерографе' `*\n",
      "def __init_(self, train_on_gc=True):\n",
      "self.train_on_gc = train_on_gc\n",
      "super().__init__()\n",
      "\n",
      "def apply_edges(self, edges):\n",
      "raise NotImplementedError\n",
      "\n",
      " \n",
      "\n",
      "def forward(self, block_outputs, pos_graph, neg_graph):\n",
      "pos_scores = {}\n",
      "with pos_graph.local_scope():\n",
      "pos_graph.ndata['h'] = block_outputs[ 'okved']\n",
      "for etype in pos_graph.canonical_etypes:\n",
      "pos_graph.apply_edges(self.apply_edges, etype=etype)\n",
      "pos_scores[etype] = pos_graph.edges[etype].data[ 'score']\n",
      "\n",
      "neg_scores = {}\n",
      "with neg_graph.local_scope():\n",
      "neg_graph.ndata[h'] = block_outputs['okved']\n",
      "for etype in neg_graph.canonical_etypes:\n",
      "neg_graph.apply_edges(self.apply_edges, etype=etype)\n",
      "neg_scores[etype] = neg_graph.edges[etype].data[ 'score' ]\n",
      "\n",
      "if self.train_on_ge:\n",
      "\n",
      " \n",
      "\n",
      "pos_score = pos_scores[('okved', 'gc', 'okved')]\n",
      "neg_score = neg_scores[('okved', 'gc', 'okved')]\n",
      "\n",
      "else:\n",
      "pos_score = torch.cat([v for v in pos_scores.values()])\n",
      "neg_score = torch.cat([v for v in neg_scores.values()])\n",
      "\n",
      "score = torch.cat([pos_score, neg_score], di\n",
      "label = torch.cat([torch.ones(len(pos_score)),\n",
      "\n",
      "torch. zeros(1en(neg_score))]).long()\n",
      "return score, label\n",
      "\n",
      " \n",
      "\n",
      "8)\n",
      "\f",
      "class HeteroDMCELoss(HeteroLoss\n",
      "\n",
      "*”`Функция потерь для предсказания связей в гетерографе с использованием скалярного произведения' '*\n",
      "output_number = True\n",
      "apply_edges = fn.u_dot_v('h', 'В', 'score')\n",
      "\n",
      " \n",
      "\n",
      "def __init_(self, emb_size, train_on_gc=True):\n",
      "super().__init__(train_on_gc)\n",
      "self.emb_size = emb_size\n",
      "self.relation = nn.Parameter(torch.rand(emb_size))\n",
      "\n",
      "def apply_edges(self, edges\n",
      "h_u = edges.src['h']\n",
      "h_y = edges.dst[h']\n",
      "г = self.relation.repeat((len(h_u), 1))\n",
      "score = torch.sum(h_u * г * ВУ, dim=1).view(-1, 1)\n",
      "return {'score': score}\n",
      "\n",
      " \n",
      "\n",
      "def forward(self, block_outputs, pos_graph, neg_graph):\n",
      "score, label = super().forward(block_outputs, pos_graph, neg_graph)\n",
      "assert score.shape[1] == 1\n",
      "\n",
      " \n",
      "\n",
      "loss = F.binary_cross_entropy_with_logits(score.flatten(), label.float(), pos_weight=torch.LongTensor([5]))\n",
      "\n",
      "return loss, score, label\n",
      "\f",
      "class HeteroMLPCELoss(nn.Module):\n",
      "*''Функция потерь для предсказания связей в гетерографе с использованием полносвязного слоя'\n",
      "output_number = False\n",
      "def __init_(self, in_features, out_classes, node_type='okved', train_on_gc=True):\n",
      "super().__init__(train_on_gc)\n",
      "self.W = nn.Linear(in_ features * 2, out_classes)\n",
      "\n",
      " \n",
      "\n",
      "def apply_edges(self, edges):\n",
      "h_u = edges.src[*h']\n",
      "Ку = edges.dst[h']\n",
      "score = self.W(torch.cat([h_u, h_v], 1))\n",
      "return {'score': score}\n",
      "\n",
      "def forward(self, block_outputs, pos_graph, neg_graph):\n",
      "score, label = super().forward(block_outputs, pos_graph, neg_graph)\n",
      "\n",
      "loss = F.cross_entropy(score, label)\n",
      "\n",
      "return loss, score, label\n",
      "\f",
      "class DotCeLossWithOkvedDistances(torch.nn.Module) :\n",
      "*''Функция потерь для бинарной классификации ребер с учетом эмбеддингов кодов ОКВЭД узлов' '*\n",
      "def __init_(self, okved_embeddings: torch.nn.Embedding, pos_weight, okved_impact: float = 1.0):\n",
      "super().__init__()\n",
      "self.okved_embeddings = okved_embeddings\n",
      "self.okved_impact = okved_impact\n",
      "self.pos_weight = pos_weight\n",
      "\n",
      " \n",
      "\n",
      "def apply_edges(self, edges):\n",
      "h_u = edges.src['h']\n",
      "\n",
      "h_y = edges.dst['h']\n",
      "\n",
      "o_u = edges.src[ 'okved']\n",
      "\n",
      "o_v = edges.dst[ 'okved']\n",
      "\n",
      "d_o = (self.okved_embeddings(o_u) - self.okved_embeddings(o_v)).pow(2).sum(dim=1).sqrt()\n",
      "dot = torch.sum(h_u * h_v, dim=1)\n",
      "\n",
      "score = dot.subtract(self.okved_impact * d_o).view(-1, 1)\n",
      "return {'score': score}\n",
      "\n",
      "def forward(self, в, В, okveds, labels, mask):\n",
      "with g.local_scope():\n",
      "g-ndata['h'] = h\n",
      "g-ndata[ 'okved'] = okveds\n",
      "g-apply_edges(self.apply_edges)\n",
      "g-ndata.pop('h')\n",
      "g-ndata.pop( 'okved')\n",
      "score = g.edata.pop( score')\n",
      "assert score.shape[1] == 1\n",
      "score_masked = score. flatten()[mask]\n",
      "labels_masked = labels.float()[mask]\n",
      "loss = F.binary_cross_entropy with_logits(score_masked, lebels_masked, pos_weight=self.pos_weight)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "return loss, score_masked, labels_masked\n",
      "\f",
      "import dgl\n",
      "import torch.nn as nn\n",
      "import dgl.nn аз gnn\n",
      "import torch\n",
      "\n",
      "class RGCN(nn.Module):\n",
      "def _init_(self, in_feats, n_hidden, n_classes, n_layers, activation, dropout, rel_names):\n",
      "super().__init__()\n",
      "self.n_layers = n_layers\n",
      "self.n_hidden = n_hidden\n",
      "self.n_classes = n_classes\n",
      "self.layers = nn.ModuleList()\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "if n_layers > 1:\n",
      "self. layers. append (gnn.HeteroGraphConv({\n",
      "rel: gnn.GraphConv(in_feats, n_hidden, norm='both')\n",
      "for rel in rel_names\n",
      "}, aggregate='sum'))\n",
      "for i in range(1, n_layers - 1):\n",
      "self. layers. append(gnn.HeteroGraphConv({\n",
      "rel: gnn.GraphConv(n_hidden, n_hidden, norm='both')\n",
      "for rel in rel_names\n",
      "}, aggregate='sum'))\n",
      "self. layers. append (gnn.HeteroGraphConv({\n",
      "rel: gnn.GraphConv(n_hidden, n_classes, norm='both')\n",
      "for rel in rel_names\n",
      "}, aggregate='sum'))\n",
      "else:\n",
      "self. layers. append(gnn.HeteroGraphConv({\n",
      "rel: gnn.GraphConv(in_feats, n_classes, norm='both')\n",
      "for rel in rel_names\n",
      "}, aggregates' sum'))\n",
      "self.dropout = nn.Dropout (dropout)\n",
      "self.activation = activation\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "def forward(self, blocks, x):\n",
      "hex\n",
      "for 1, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
      "h = layer(block, h)\n",
      "if 1 != len(self.layers) - 1:\n",
      "{k: self.activation(v) for k, v in h.items()}\n",
      "{k: self.dropout(v) for К, v in h.items()}\n",
      "\n",
      " \n",
      "\f",
      "def get_embeddings(self, g):\n",
      "”'Инференс без использования сэмплинга соседей\n",
      "x = {'okved': g.ndata[ 'features' ].float()}\n",
      "for 1, layer in enumerate(self.layers):\n",
      "y = {'okved': torch.zeros(g.number_of_nodes('okved'),\n",
      "self.n_hidden if 1 != len(self.layers)-1 else self.n_classes)}\n",
      "\n",
      " \n",
      "\n",
      "sampler = dgl.dataloading.MultiLayerFul1NeighborSampler(1)\n",
      "dataloader = dgl.dataloading.NodeDataLoader(g,\n",
      "{'okved': torch.arange(g-num_nodes()).to(g-device)},\n",
      "sampler,\n",
      "batch_size=g.num_nodes(),\n",
      "shuffle=False,\n",
      "drop_last=False,\n",
      "num_workers=1)\n",
      "for input_nodes, output_nodes, blocks in dataloader:\n",
      "block = blocks[@].int().to(g.device)\n",
      "В = {'okved': x['okved' ][input_nodes].to(g.device)}\n",
      "h = layer(block, h)\n",
      "if 1 != len(self.layers) - 1:\n",
      "h = {k: self.activation(v) for k, v in h.items()}\n",
      "h = {k: self.dropout(v) for К, у in h.items()}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "yL'okved' ][output_nodes] = h['okved'].cpu()\n",
      "=F\n",
      "\n",
      "return y\n",
      "\f",
      "import torch.nn as пп\n",
      "import torch\n",
      "import dgl.nn as gnn\n",
      "\n",
      "class SAGE(nn.Module):\n",
      "def _ init__(self, in_feats, n_hidden, n_classes, n_layers,\n",
      "\n",
      "activation, dropout,\n",
      "freeze, embedding tensor):\n",
      "\n",
      "super().__init__()\n",
      "\n",
      "self.n_layers = n_layers\n",
      "\n",
      "self.n_hidden = n_hidden\n",
      "\n",
      "self.n_classes = n_classes\n",
      "\n",
      "self.layers = nn.ModuleList()\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "self.freeze = freeze\n",
      "self.embeddings = nn. Embedding. from_pretrained(embedding tensor, freeze=self. freeze)\n",
      "\n",
      " \n",
      "\n",
      "if n_layers > 1:\n",
      "self. layers. append(gnn.SAGEConv(in_feats, n_hidden, aggregator_type='mean', activation=activation, feat_drop=dropout))\n",
      "for i in range(1, n_layers - 1):\n",
      "\n",
      "self. layers. append(gnn.SAGEConv(n_hidden, n_hidden, aggregator_type='mean', activation=activation, feat_drop=dropout))\n",
      "self. layers. append(gnn.SAGEConv(n_hidden, n_classes, aggregator_type='mean'))\n",
      "\n",
      "else:\n",
      "self. layers. append(gnn.SAGEConv(in_feats, n_classes, aggregator_typ:\n",
      "\n",
      "self.dropout = nn.Dropout (dropout)\n",
      "\n",
      "self.activation = activation\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "mean'))\n",
      "\n",
      "def forward(self, g, x, okveds):\n",
      "# получаем эмбеддинги ОКВЭД, конкатенируем с остальными фичами узлов и прогоняем через сеть\n",
      "okved_embs = self. embeddings (оКуе45)\n",
      "h = torch.cat([x, okved_embs], dim=1)\n",
      "for 1, (layer) in enumerate(self. layers):\n",
      "h = layer(g, h)\n",
      "return h\n",
      "\f",
      "import dgl\n",
      "\n",
      "class NegativeHeteroGraphSampler (object) :\n",
      "''*PeanwayeT негативное сэмплирование' '*\n",
      "def п! (зе1%, в, neg_examples: int, gamma: float = -0.75) -> None:\n",
      "self.weights = sum(g.in_degrees(etype=etype) for etype in g.canonical_etypes).float() ** gamma\n",
      "self.neg_examples = neg_examples\n",
      "\n",
      "def _call_(self, в, eids_dict: dict) -> dict:\n",
      "\n",
      "result_dict = {}\n",
      "\n",
      "for etype, eids in eids_dict.items():\n",
      "src, _ = g.find_edges(eids, etype=etype)\n",
      "п = len(src)\n",
      "dst = self.weights.multinomial(n*self.neg examples, replacement=True)\n",
      "src = src.repeat_interleave(self.neg_examples)\n",
      "result_dict[etype] = (src, dst)\n",
      "\n",
      "return result_dict\n",
      "\f",
      " \n",
      "\n",
      "@1_preprocessing. ipynb\n",
      "@2_okved_embeddings_graph. ipynb\n",
      "@3_okved_embeddings_model.ipynb\n",
      "04_profit_classification.ipynb\n",
      "@5_company_struct_embeddings.ipynb\n",
      "CONFIG. yam\n",
      "\n",
      "encoders.py\n",
      "\n",
      "imports. py\n",
      "\n",
      "README .md\n",
      "\n",
      "| Чака\n",
      "|—additional\n",
      "\n",
      "bankrupt_arango_gsk.csv\n",
      "bankrupt_arango_gsk. preprocessed. csv\n",
      "company inout_graphs.pickle\n",
      "company_okved_dict.pickle\n",
      "fin_arango_gsk.7z\n",
      "\n",
      "fin_arango_gsk.csv\n",
      "in_arango_gsk.preprocessed.csv\n",
      "okved_data.csv\n",
      "\n",
      "okved_graph. pickle\n",
      "\n",
      "profits_dataset.csv\n",
      "\n",
      "Документ предоставлен КонсультантПлюс .docx\n",
      "\n",
      "[—demo\n",
      "дето1 .х15х\n",
      "дето2 .х15х\n",
      "metrics_profit_classification.pickle\n",
      "METRIC_DONT_REWRITE. pickle\n",
      "METRIC_DONT_REWRITE2. pickle\n",
      "profit_model_@\n",
      "profit_model_1\n",
      "profit_model_2\n",
      "\n",
      "[embeddings\n",
      "company_embeddings.pickle\n",
      "okved_embeddings. pickle\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "гск уз\n",
      "'_rck_v3_augmented\n",
      "—models\n",
      "loss. py\n",
      "rgcn.py\n",
      "sage. py\n",
      "samplers.py\n",
      "Зи „ру\n",
      "'checkpoints\n",
      "\n",
      "company _embedding model. pth\n",
      "okved_embedding_model.pth\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(code.replace('‘', \"'\").replace('\"', \"'\").replace('’', \"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a54973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
