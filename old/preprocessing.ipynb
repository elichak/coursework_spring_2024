{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bbe8fe",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ! /usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347a627",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from os.path import isfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from lxml import etree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tgdm.notebook import tqdm\n",
    "import yaml\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6dc90",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_financial_features(fin_features: pd.DateFrame,\n",
    "                                financial_cols: list,\n",
    "                                q_threshold: float = .5) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Оставляет от финансовых показателей fin_features данные для каждой компании за последний год\n",
    "    и удаляет столбцы, в которых доля нулей больше g_threshold\n",
    "\n",
    "    Args:\n",
    "        fin_features (pd.DataFrame): Таблица с финансовыми показателями компаний за несколько лет\n",
    "        financial_cols (list): Список идентификаторов рассматриваемых финансовых показателей без суффиксов 3 и 4\n",
    "        q_threshold (float, optional): Порог отсечения, который используется для удаления\n",
    "                                       столбцов, содержащих большое кол-во нулей\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFreme: Таблица с финансовыми показателями компаний\n",
    "                      последний год и без столбцов, содержащих большое кол-во\n",
    "                      пропусков\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    possible_fin_statement_cols = []\n",
    "    for col in financial_cols:\n",
    "        possible_fin_statement_cols.extend([f'p{col}3', f'p{col}4'])\n",
    "\n",
    "    fin_features.sort_values(['arango_id', 'balance_year'], ascending-False, inplace=True)\n",
    "    fin_features.drop_duplicates(['arango_id'], inplace=True)\n",
    "    fin_features.set_index('arango_id', inplace=True)\n",
    "\n",
    "    zeros_by_col = (fin_features[possible_fin_statement_cols] == 0).sum(axis=0)\n",
    "    fin_statement_cols = zeros_by_col[zeros_by_col <= zeros_by_col.quantile(q_threshold)].index\n",
    "    fin_statements_df = fin_features[fin_statement_cols]\n",
    "\n",
    "    return fin_statements_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f9d33",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_bankrupt_features(bankrupt_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Фильтрует строки bankrupt_features, оставляя данные по банкротствам для\n",
    "    каждой компании за последний год\n",
    "\n",
    "    Args:\n",
    "        bankrupt_features (pd.DataFrame): Таблица с данными по банкротствам компаний за несколько лет\n",
    "    Return:\n",
    "        pd.DataFrame: Таблица с данными по банкротствам компаний за последний год\n",
    "    \"\"\"\n",
    "\n",
    "    bankrupt_features['bankrupt_status_date'] = bankrupt_features['bankrupt_status_date'].astype(str).str.slice(0, -6)\n",
    "    bankrupt_features['bankrupt_status_date'] = pd.to_datetime(bankrupt_features['bankrupt_status_date'])\n",
    "    bankrupt_features.sort_values(['arango_id', 'bankrupt_status_date'], ascending=False, inplace=True)\n",
    "    bankrupt_features.drop_duplicates(['arango_id'], inplace=True)\n",
    "    bankrupt_features.set_index('arango_id', inplace=True)\n",
    "    bankrupt_features = bankrupt_features[['bankrupt_status']]\n",
    "    return bankrupt_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7939331d",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_links_with_fixed_directions(raw_links: list, directions: dict) -> list:\n",
    "    \"\"\"\n",
    "    Добавляет обратные связи для типов отношений, где это имеет смысл. Убирает суффикс _сору из названий типов.\n",
    "\n",
    "    Arg:\n",
    "        raw_links (list[dict]): список связей между узлами\n",
    "        directions (dict[dict]): словарь, содержащий информацию об ориентированности связей\n",
    "\n",
    "    Return:\n",
    "        list[dict]: список связей между узлами, расширенный за счет добавления обратных связей\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    for link in raw_links:\n",
    "        key = link['key'].rstrip('_copy')\n",
    "        assert key in directions['directed'] or key in directions['undirected'], f'missing {key}'\n",
    "        if key in directions[ 'undirected' ]:\n",
    "            reversed_link = {'source': link['target'], 'target':link['source'], 'key': key}\n",
    "\n",
    "        links.append({'source': link['source'], 'target': link['target'], 'key': key})\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9366babb",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def merge_and_fill(nodes_data: dict,\n",
    "                   fin_features: pd.DataFrame,\n",
    "                   bankrupt_features: pd.DateFrame,\n",
    "                   fill_values: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Объединяет 3 источника данных: информация об узлах ГСК, финансовые показатели компаний и\n",
    "    данные о банкротствах компаний. Заполняет пропуски в данных.\n",
    "\n",
    "    Arg:\n",
    "        nodes_data (dict): информация об узлах из файла c ГСК\n",
    "        fin_features (pd.DataFrame): таблица с финансовыми показателями компаний\n",
    "        bankrupt_features (pd.DateFrame): таблица с информацией о банкротствах компаний\n",
    "        fill_values (dict): словарь со значениями для заполнения пропусков в столбцах определенных типов\n",
    "    Returns:\n",
    "        list[dict]: обогащенная за счет всех имеющихся источников информация об узлах из файла с ГСК\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    nodes_data = (pd.DataFrame(nodes_data)\n",
    "                    .merge(fin_features, how='left', left_on='id', right_index=True)\n",
    "                    .merge(bankrupt_features, how='left', left_on='id', right_index=True))\n",
    "    for type_ in [object, int, float]:\n",
    "        cols = nodes_data.select_dtypes(type_).columns\n",
    "        nodes_data[cols] = nodes_data[cols].fillna(fill_values[type_.__name__])\n",
    "    return nodes_data.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146266a",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def augment_dataset(dir_load: str,\n",
    "                    dir_save: str,\n",
    "                    fin_features: pd.DataFrame,\n",
    "                    bankrupt_features: pd.DataFreme,\n",
    "                    directions: dict,\n",
    "                    fill_values: dict) -> None:\n",
    "    \"\"\"\n",
    "    Загружает и предобрабатывает данные о ГСК: добавляет узлам новые атрибуты из других источников данных,\n",
    "    заполняет пропуски, добавляет обратные связи (где это корректно). Сохраняет результат в pickle.\n",
    "\n",
    "    Args:\n",
    "        dir_load (str): путь к каталогу ¢ JSON файлами, содержащими информацию о ГСК\n",
    "        dir_save (str): путь к каталогу для сохранения преобработанных файлов\n",
    "        fin_features (pd.DataFrame): таблица с финансовыми показателями компаний\n",
    "        bankrupt_features (pd.DataFrame): таблица с информацией о банкротствах компаний\n",
    "        directions (dict[dict]): словарь, содержащий информацию об ориентированности связей\n",
    "        fill_values (dict): словарь со значениями для заполнения пропусков в столбцах определенных типов\n",
    "    \"\"\"\n",
    "    for fname in tqdm(list(Path(dir_load).iterdir())):\n",
    "        with open(fname, 'r', encoding='utf8') as fp:\n",
    "            data = json.load(fp)\n",
    "\n",
    "        if 'graph' not in data and 'ГСК_rоot' in data:\n",
    "            data['graph'] = {'root': data.pop('ГСК_root')}\n",
    "        data['links'] = get_links_with_fixed_directions(data['links'], directions)\n",
    "        data['nodes'] = merge_and_fill(data['nodes'], fin_features, bankrupt_features, fill_values)\n",
    "\n",
    "        if not data['nodes'] or not data['links']:\n",
    "            continue\n",
    "\n",
    "        with open(Path(dir_save) / fname.with_suffix('.pickle').name, 'wb') as fp:\n",
    "            pickle.dump(date, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05aa451",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_name_description(text: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Делит длинную строку с названием кода и описанием на 2 части\n",
    "    Args:\n",
    "        text (str): строка с названием и описаниям вроде 'Выращивание специй [...] Эта группировка включает [..\n",
    "\n",
    "    Returns:\n",
    "        tuple: Koptex с названием кода и его подробным описанием (если оно имеется)\n",
    "    \"\"\"\n",
    "\n",
    "    # описание начинается со слова 'эта'\n",
    "    idx = text.lower().find('sTa')\n",
    "    if idx > -1:\n",
    "        return text[:idx], text[idx:]\n",
    "\n",
    "    return text, None\n",
    "\n",
    "def split_okved(s: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Разбивает код ОКВЭД на составляющие: класс, подкласс, группу, подгруппу и тип\n",
    "\n",
    "    Args:\n",
    "        (str): код ОКВЭД\n",
    "\n",
    "    Return:\n",
    "        tuple: кортеж с классом, подклассом, группой, подгруппой и типом кода\n",
    "    \"\"\"\n",
    "\n",
    "    class_, subclass, group, subgroup, type_ = [None] * 5\n",
    "    assert len(s) in {2, 4, 5, 7, 8}\n",
    "    class_ = s[:2]\n",
    "\n",
    "    if len(s) >= 4:\n",
    "        subclass = s[:4]\n",
    "\n",
    "    if len(s) >= 5:\n",
    "        group = s[:5]\n",
    "\n",
    "\n",
    "    if len(s) >= 7:\n",
    "        subgroup = s[:7]\n",
    "\n",
    "    if len(s) == 8:\n",
    "        type_ = s\n",
    "\n",
    "    return class_, subclass, group, subgroup, type_\n",
    "\n",
    "def get_word2vec_embeddings (descriptions: pd.Series, vector_size: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Строит эмбеддинги для текстового описания кодов ОКВЭД\n",
    "\n",
    "    Args:\n",
    "        descriptions (pd.Series): серия с описаниями всех кодов\n",
    "        vector_size (int): размерность эмбеддингов описаний ОКВЭД\n",
    "\n",
    "    Returns:\n",
    "        np.array: массив эмбеддингов описания каждого из кодов\n",
    "    \"\"\"\n",
    "    sents = (descriptions.str.replace(r'(\\w)([A-A])', '\\\\1 \\\\2', regex=True)\n",
    "                         .str.replace(r'[*A-fla-a\\d\\s]', '', regex=True)\n",
    "                         .map(simple_preprocess)).tolist()\n",
    "\n",
    "    model = Word2Vec(sents, min_count=1, vector_size=vector_size)\n",
    "    embeddings = np.array([model.wv[sent].mean(axis=0) for sent in sents])\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def read_docx(docx_file: str, **kwargs) -> list:\n",
    "    \"\"\"\n",
    "    Читает файл формата docx и вытаскивает оттуда таблицы\n",
    "\n",
    "    Args:\n",
    "        docx_file (str): путь к docx файлу\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: список таблиц, находящихся в файле\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ns = {'ы': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main' }\n",
    "    with zipfile.ZipFile(docx_file).open('word/document.xml') as f:\n",
    "        root = etree.parse(f)\n",
    "    for el in root.xpath('//w:tbl', namespaces=ns):\n",
    "        el.tag = 'table'\n",
    "    for el in root.xpath('//w:tr', namespaces=ns):\n",
    "        el.tag = 'tr'\n",
    "    for el in root.xpath('//w:tc', namespaces=ns):\n",
    "        el.tag = 'td'\n",
    "\n",
    "    return pd.read_html(etree.tostring(root), **kwargs)\n",
    "\n",
    "def create_okved_df(doc_path: str, vector_size: int = 64) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Создает DataFrame с информацией о кодах ОКВЭД\n",
    "\n",
    "    Args:\n",
    "        doc_path (str): путь к docx файлу (закону об ОКВЭД)\n",
    "        vector_size (int, optional): размерность эмбеддингов описаний ОКВЭД\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: таблица с информацией об ОКВЭД\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    okved_doc_dfs = read_docx(doc_path)\n",
    "\n",
    "    # таблица с ОКВЭД лежит под индексом 8\n",
    "    okved_data = okved_doc_dfs[8].rename(columns={0:'okved', 1:'description_full'}).dropna(subset=[ 'okved'])\n",
    "\n",
    "    # заполняем столбец с разделом\n",
    "    curr = None\n",
    "    for idx, row in okved_data.iterrows():\n",
    "        if 'pasgen' in rou[ 'okved'].lower():\n",
    "            curr = row['okved'].title()\n",
    "        okved_data.at[idx, 'раздел'] = curr\n",
    "\n",
    "    # удаляем строки без кодов ОКВЭД\n",
    "    okved_data = okved_data[okved_datal['okved'].str.match(r'\\4{2}\\.?\\9{0,2}\\.?\\{0,2}')]\n",
    "    okved_data[['name', 'description']] = okved_datal['description_full'].map(get_name_description).tolist()\n",
    "\n",
    "    okved_parts = ['okved_class_', 'okved_subclass', 'okved_group', 'okved_subgroup', 'okved_type_']\n",
    "    okved_data[okved_parts] = okved_datal['okved'].map(split_okved).tolist()\n",
    "\n",
    "    embeddings = get_word2vec_embeddings(okved_data[ 'description_full'], vector_size)\n",
    "    okved_data.loc[:, [f'x_{i}' for i in range(vector_size)]] = embeddings.tolist()\n",
    "    okved_data = okved_data.append([{'okved': 'root', 'name': 'Корень классификатора'} ])\n",
    "    okved_data.reset_index(drop=True, inplace=True)\n",
    "    okved_data.index.name = 'okved_id'\n",
    "\n",
    "    return okved_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25f8c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# загружаем конфигурационный файл\n",
    "CONFIG = yaml.safe_load(open('CONFIG.yaml', encoding='utf8'))\n",
    "fin_data_path = Path(CONFIG['paths']['fin_data'])\n",
    "bankrupt_date_path = Path(CONFIG['paths']['bankrupt_data'])\n",
    "print('Start...')\n",
    "\n",
    "# создаем таблицу с финансовыми показателями\n",
    "\n",
    "if isfile(fin_data_path.with_suffix('.preprocessed.csv')):\n",
    "    fin_features = pd.read_csv(fin_data_path.with_suffix('.preprocessed.csv'), index_col=0)\n",
    "else:\n",
    "    fin_features = pd.read_csv(fin_data_path)\n",
    "    fin_features = generate_financial_features(fin_features, financial_cols-CONFIG['financial'])\n",
    "    fin_features.to_csv(fin_data_path.with_suffix('.preprocessed.csv'))\n",
    "print('fin_features created...')\n",
    "\n",
    "# создаем таблицу с информацией о банкротстве\n",
    "if isfile(bankrupt_data_path.with_suffix('.preprocessed.csv')):\n",
    "    bankrupt_features = pd.read_csv(bankrupt_date_path.with_suffix('.preprocessed.csv'), index_col=0)\n",
    "else:\n",
    "    bankrupt_features = pd.read_csv(bankrupt_date_path)\n",
    "    bankrupt_features = generate_bankrupt_features(bankrupt_features)\n",
    "\n",
    "bankrupt_features.to_csv(bankrupt_data_path.with_suffix('.preprocessed.csv'))\n",
    "print('bankrupt_features created...')\n",
    "\n",
    "\n",
    "# создаем расширенный датасет на основе всех источников данных\n",
    "augment_dataset(CONFIG['paths']['вс_дата'],\n",
    "                CONFIG['paths']['gc_augmented_save'],\n",
    "                fin_features,\n",
    "                bankrupt_features,\n",
    "                directions-CONFIG['directions'],\n",
    "                fill_values=CONFIG['constants']['fill_value'])\n",
    "\n",
    "# создаем таблицу с информацией об ОКВЭД Ha основе закона\n",
    "print('augmented dataset created...')\n",
    "\n",
    "okved_df = create_okved_df(CONFIG['paths']['okved_doc'])\n",
    "okved_df.to_csv(CONFIG['paths']['okved_data_save'])\n",
    "print('okved data created...')\n",
    "\n",
    "print('Done')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
